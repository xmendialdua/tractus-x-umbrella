# Solución de Errores en Keycloak - Cookie not found y HTTPS required

**Fecha:** 26 de enero de 2026  
**Entorno:** Despliegue en OVH con IP LoadBalancer: 51.68.114.44  
**Namespace:** portal

---

## Resumen Ejecutivo

Durante el despliegue del portal Catena-X en OVH, se encontraron dos problemas críticos al acceder a las consolas de administración de Keycloak (Central IDP y Shared IDP):

1. **Error "Cookie not found"**: Impedía el login en la consola de administración
2. **Error "HTTPS required"**: Bloqueaba el acceso debido a que los realms estaban configurados para requerir HTTPS

Este documento detalla los problemas encontrados, su causa raíz y las soluciones implementadas paso a paso.

---

## Contexto del Despliegue

### Comando de Instalación Inicial
```bash
helm install portal . \
  -f values-adopter-portal.yaml \
  -f values-ovh-hosts-portal.yaml \
  -n portal \
  --create-namespace
```

### URLs del Entorno
- Portal: http://portal.51.68.114.44.nip.io
- Central IDP: http://centralidp.51.68.114.44.nip.io/auth/
- Shared IDP: http://sharedidp.51.68.114.44.nip.io/auth/
- Load Balancer IP: 51.68.114.44

---

## Problema 1: Cookie not found

### Síntomas
Al intentar acceder a la consola de administración de Keycloak en:
- `http://centralidp.51.68.114.44.nip.io/auth/`

Después de introducir las credenciales (usuario: `admin`), aparecía el mensaje:
```
We are sorry...
Cookie not found. Please make sure cookies are enabled in your browser.
```

### Diagnóstico

#### Verificación de Logs
```bash
kubectl logs portal-centralidp-0 -n portal | grep -i cookie
```

**Resultado:**
```
type="LOGIN_ERROR", realmName="master", error="cookie_not_found"
```

#### Verificación de Variables de Entorno
```bash
kubectl describe pod portal-centralidp-0 -n portal | grep -A 10 "Environment:"
```

Se encontró que faltaban variables de entorno críticas para la configuración de hostname y proxy.

### Causa Raíz

Keycloak requiere configuración específica cuando se ejecuta detrás de un proxy/ingress y accede a través de un hostname diferente al interno. La configuración inicial carecía de las siguientes variables de entorno esenciales:

- `KC_HOSTNAME_STRICT`: Control estricto de hostname
- `KC_HOSTNAME_STRICT_HTTPS`: Requerimiento estricto de HTTPS
- `KC_PROXY`: Modo de proxy (edge, reencrypt, passthrough)
- `KC_HOSTNAME`: Hostname público
- `KC_HOSTNAME_PATH`: Path base de la aplicación

Sin estas configuraciones, Keycloak no podía manejar correctamente las cookies cuando las solicitudes venían a través del nginx-ingress.

### Solución Implementada

#### Paso 1: Modificar el archivo de valores

**Archivo:** `charts/umbrella/values-ovh-hosts-portal.yaml`

Se agregaron las siguientes variables de entorno en las secciones `centralidp.keycloak.extraEnvVars` y `sharedidp.keycloak.extraEnvVars`:

```yaml
centralidp:
  keycloak:
    extraEnvVars:
      # Variables existentes
      - name: KEYCLOAK_FRONTEND_URL
        value: "http://centralidp.51.68.114.44.nip.io/auth/"
      - name: KEYCLOAK_PROXY_ADDRESS_FORWARDING
        value: "true"
      - name: KC_SPI_STICKY_SESSION_ENCODER_INFINISPAN_SHOULD_ATTACH_ROUTE
        value: "false"
      - name: KEYCLOAK_PRODUCTION
        value: "false"
      - name: KC_HTTP_ENABLED
        value: "true"
      
      # NUEVAS VARIABLES AGREGADAS
      - name: KC_HOSTNAME_STRICT
        value: "false"
      - name: KC_HOSTNAME_STRICT_HTTPS
        value: "false"
      - name: KC_PROXY
        value: "edge"
      - name: KC_HOSTNAME
        value: "centralidp.51.68.114.44.nip.io"
      - name: KC_HOSTNAME_PATH
        value: "/auth/"
```

**Mismo cambio aplicado para sharedidp:**
```yaml
sharedidp:
  keycloak:
    extraEnvVars:
      # ... (variables existentes)
      
      # NUEVAS VARIABLES AGREGADAS
      - name: KC_HOSTNAME_STRICT
        value: "false"
      - name: KC_HOSTNAME_STRICT_HTTPS
        value: "false"
      - name: KC_PROXY
        value: "edge"
      - name: KC_HOSTNAME
        value: "sharedidp.51.68.114.44.nip.io"
      - name: KC_HOSTNAME_PATH
        value: "/auth/"
```

#### Paso 2: Actualizar el despliegue con Helm

**IMPORTANTE:** Usar ambos archivos de valores como en el despliegue inicial:

```bash
cd ~/projects/assembly/tractus-x-umbrella/charts/umbrella

helm upgrade portal . \
  -f values-adopter-portal.yaml \
  -f values-ovh-hosts-portal.yaml \
  -n portal
```

**Nota:** Si solo usas `-f values-ovh-hosts-portal.yaml`, los componentes de Keycloak no se desplegarán porque necesitan la configuración base de `values-adopter-portal.yaml`.

#### Paso 3: Verificar la actualización

```bash
# Ver el estado del release
helm status portal -n portal

# Verificar que los pods se están recreando
kubectl get pods -n portal -w

# Verificar las nuevas variables de entorno
kubectl describe pod portal-centralidp-0 -n portal | grep -E "KC_HOSTNAME|KC_PROXY"
```

**Salida esperada:**
```
KC_HOSTNAME_STRICT:         false
KC_HOSTNAME_STRICT_HTTPS:   false
KC_PROXY:                   edge
KC_HOSTNAME:                centralidp.51.68.114.44.nip.io
KC_HOSTNAME_PATH:           /auth/
```

#### Paso 4: Verificar que el error se resolvió

```bash
# Ver logs recientes (no debería aparecer cookie_not_found)
kubectl logs portal-centralidp-0 -n portal --tail=50 | grep -i cookie

# Probar conectividad
curl -I http://centralidp.51.68.114.44.nip.io/auth/
```

**Resultado exitoso:**
```
HTTP/1.1 302 Found
Set-Cookie: route=....; Path=/auth/; HttpOnly
```

---

## Problema 2: HTTPS required

### Síntomas

Después de resolver el problema de cookies, al acceder a:
- `http://centralidp.51.68.114.44.nip.io/auth/`

Aparecía el mensaje:
```
We are sorry...
HTTPS required
```

### Diagnóstico

Este error indica que los realms de Keycloak están configurados para requerir HTTPS, pero el acceso se realiza por HTTP (ya que el entorno usa `tls: false` con nip.io).

### Causa Raíz

En la base de datos de Keycloak, cada realm tiene una columna `ssl_required` que puede tener los valores:
- `ALL`: Requiere HTTPS para todas las conexiones (valor por defecto)
- `EXTERNAL`: Requiere HTTPS solo para conexiones externas
- `NONE`: No requiere HTTPS

Los realms `master`, `CX-Central` y `CX-Operator` estaban configurados con `ssl_required = 'ALL'` o `EXTERNAL`, pero el entorno no tiene HTTPS configurado.

### Solución Implementada

#### Paso 1: Crear Job de Kubernetes para actualizar la base de datos

**Archivo creado:** `charts/umbrella/fix-https-requirement-job.yaml`

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: fix-https-requirement
  namespace: portal
spec:
  template:
    spec:
      containers:
      - name: fix-https
        image: postgres:15-alpine
        command:
          - /bin/sh
          - -c
          - |
            echo "=========================================="
            echo "Fixing HTTPS requirement in Keycloak realms"
            echo "=========================================="
            
            export PGPASSWORD="${POSTGRES_PASSWORD_CENTRAL}"
            
            echo "=== Updating Central IDP realm SSL settings ==="
            psql -h portal-centralidp-postgresql -U kccentral -d iamcentralidp <<EOF
            UPDATE realm SET ssl_required = 'NONE' WHERE name = 'master';
            UPDATE realm SET ssl_required = 'NONE' WHERE name = 'CX-Central';
            SELECT name, ssl_required FROM realm WHERE name IN ('master', 'CX-Central');
            EOF
            
            export PGPASSWORD="${POSTGRES_PASSWORD_SHARED}"
            
            echo ""
            echo "=== Updating Shared IDP realm SSL settings ==="
            psql -h portal-sharedidp-postgresql -U kcshared -d iamsharedidp <<EOF
            UPDATE realm SET ssl_required = 'NONE' WHERE name = 'master';
            UPDATE realm SET ssl_required = 'NONE' WHERE name = 'CX-Operator';
            SELECT name, ssl_required FROM realm WHERE name IN ('master', 'CX-Operator');
            EOF
            
            echo ""
            echo "=========================================="
            echo "HTTPS requirement disabled successfully!"
            echo "=========================================="
        env:
        - name: POSTGRES_PASSWORD_CENTRAL
          valueFrom:
            secretKeyRef:
              name: portal-centralidp-postgresql
              key: password
        - name: POSTGRES_PASSWORD_SHARED
          valueFrom:
            secretKeyRef:
              name: portal-sharedidp-postgresql
              key: password
      restartPolicy: Never
  backoffLimit: 3
```

**Explicación del Job:**
- Usa la imagen `postgres:15-alpine` que incluye el cliente psql
- Se conecta a las bases de datos PostgreSQL de Central IDP y Shared IDP
- Actualiza la columna `ssl_required` a `NONE` para los realms relevantes
- Usa los secrets de Kubernetes para obtener las contraseñas de forma segura
- Verifica los cambios con un SELECT

#### Paso 2: Obtener información de los secrets (solo para referencia)

```bash
# Ver el usuario de PostgreSQL para Central IDP
kubectl get secret portal-centralidp-postgresql -n portal -o jsonpath='{.data}' | jq

# Obtener contraseña de Central IDP
kubectl get secret portal-centralidp-postgresql -n portal -o jsonpath='{.data.password}' | base64 -d

# Obtener contraseña de Shared IDP
kubectl get secret portal-sharedidp-postgresql -n portal -o jsonpath='{.data.password}' | base64 -d
```

**Nota:** Los usuarios y nombres de bases de datos se pueden verificar en los descriptores de los pods:
```bash
kubectl describe pod portal-centralidp-0 -n portal | grep -A 20 "Environment:"
```

**Información encontrada:**
- Central IDP:
  - Host: `portal-centralidp-postgresql`
  - Usuario: `kccentral`
  - Base de datos: `iamcentralidp`
  - Secret: `portal-centralidp-postgresql` (key: password)

- Shared IDP:
  - Host: `portal-sharedidp-postgresql`
  - Usuario: `kcshared`
  - Base de datos: `iamsharedidp`
  - Secret: `portal-sharedidp-postgresql` (key: password)

#### Paso 3: Aplicar el Job

```bash
kubectl apply -f ~/projects/assembly/tractus-x-umbrella/charts/umbrella/fix-https-requirement-job.yaml
```

#### Paso 4: Monitorear la ejecución del Job

```bash
# Ver el estado del Job
kubectl get jobs -n portal

# Ver los logs del Job
kubectl logs job/fix-https-requirement -n portal
```

**Salida esperada:**
```
==========================================
Fixing HTTPS requirement in Keycloak realms
==========================================

=== Updating Central IDP realm SSL settings ===
UPDATE 1
UPDATE 1
    name    | ssl_required
------------+--------------
 master     | NONE
 CX-Central | NONE

=== Updating Shared IDP realm SSL settings ===
UPDATE 1
UPDATE 1
     name     | ssl_required
--------------+--------------
 master       | NONE
 CX-Operator  | NONE

==========================================
HTTPS requirement disabled successfully!
==========================================
```

#### Paso 5: Reiniciar los pods de Keycloak

Los cambios en la base de datos no se aplican automáticamente. Necesitas reiniciar los pods:

```bash
# Reiniciar Central IDP
kubectl delete pod portal-centralidp-0 -n portal

# Reiniciar Shared IDP
kubectl delete pod portal-sharedidp-0 -n portal

# Verificar que los nuevos pods están ejecutándose
kubectl get pods -n portal | grep -E "centralidp|sharedidp"
```

**Salida esperada:**
```
portal-centralidp-0                1/1     Running   0          2m
portal-sharedidp-0                 1/1     Running   0          2m
```

#### Paso 6: Verificar el acceso

```bash
# Probar conectividad a Central IDP
curl -I http://centralidp.51.68.114.44.nip.io/auth/admin/master/console/

# Probar conectividad a Shared IDP
curl -I http://sharedidp.51.68.114.44.nip.io/auth/admin/master/console/
```

---

## Verificación Final

### Accesos Funcionando Correctamente

1. **Portal Principal:**
   - URL: http://portal.51.68.114.44.nip.io
   - Estado: ✅ Funcionando

2. **Central IDP - Consola de Administración:**
   - URL: http://centralidp.51.68.114.44.nip.io/auth/
   - Usuario: `admin`
   - Contraseña: (obtener del secret)
   - Estado: ✅ Funcionando (sin error de cookies ni HTTPS)

3. **Shared IDP - Consola de Administración:**
   - URL: http://sharedidp.51.68.114.44.nip.io/auth/
   - Usuario: `admin`
   - Contraseña: (obtener del secret)
   - Estado: ✅ Funcionando (sin error de cookies ni HTTPS)

### Comandos de Verificación Rápida

```bash
# Ver estado de todos los componentes
kubectl get pods -n portal

# Ver el estado del ingress
kubectl get ingress -n portal

# Ver la IP externa del Load Balancer
kubectl get svc -n ingress-nginx

# Obtener contraseña de admin para Central IDP
kubectl get secret portal-centralidp -n portal -o jsonpath='{.data.admin-password}' | base64 -d && echo

# Obtener contraseña de admin para Shared IDP
kubectl get secret portal-sharedidp -n portal -o jsonpath='{.data.admin-password}' | base64 -d && echo
```

---

## Archivos Modificados

### 1. values-ovh-hosts-portal.yaml
**Ubicación:** `charts/umbrella/values-ovh-hosts-portal.yaml`

**Cambios realizados:**
- Agregadas variables de entorno en `centralidp.keycloak.extraEnvVars`:
  - KC_HOSTNAME_STRICT
  - KC_HOSTNAME_STRICT_HTTPS
  - KC_PROXY
  - KC_HOSTNAME
  - KC_HOSTNAME_PATH

- Agregadas variables de entorno en `sharedidp.keycloak.extraEnvVars`:
  - KC_HOSTNAME_STRICT
  - KC_HOSTNAME_STRICT_HTTPS
  - KC_PROXY
  - KC_HOSTNAME
  - KC_HOSTNAME_PATH

### 2. fix-https-requirement-job.yaml (nuevo)
**Ubicación:** `charts/umbrella/fix-https-requirement-job.yaml`

**Descripción:** Job de Kubernetes para actualizar la configuración SSL en las bases de datos de Keycloak.

---

## Comandos de Troubleshooting

### Ver logs de Keycloak
```bash
# Central IDP
kubectl logs portal-centralidp-0 -n portal --tail=100

# Shared IDP
kubectl logs portal-sharedidp-0 -n portal --tail=100

# Buscar errores específicos
kubectl logs portal-centralidp-0 -n portal | grep -i error
```

### Verificar configuración de base de datos
```bash
# Conectarse manualmente a PostgreSQL (requiere contraseña)
kubectl exec -it portal-centralidp-postgresql-0 -n portal -- psql -U kccentral -d iamcentralidp

# Dentro de psql:
SELECT name, ssl_required FROM realm;
```

### Ver configuración de pods
```bash
# Ver todas las variables de entorno
kubectl describe pod portal-centralidp-0 -n portal

# Ver solo configuración de Keycloak
kubectl get pod portal-centralidp-0 -n portal -o yaml | grep -A 30 env:
```

### Reiniciar todos los componentes de Keycloak
```bash
kubectl delete pod -n portal -l app.kubernetes.io/name=centralidp
kubectl delete pod -n portal -l app.kubernetes.io/name=sharedidp
```

---

## Lecciones Aprendidas

### 1. Configuración de Proxy en Keycloak

Cuando Keycloak se ejecuta detrás de un proxy/ingress, es **crítico** configurar:
- `KC_PROXY=edge`: Indica que Keycloak está detrás de un proxy que termina TLS
- `KC_HOSTNAME`: El hostname público que ven los usuarios
- `KC_HOSTNAME_PATH`: El path base si Keycloak no está en la raíz

### 2. SSL/TLS en Entornos de Desarrollo

Para entornos sin HTTPS (como nip.io):
- Configurar `KC_HOSTNAME_STRICT=false`
- Configurar `KC_HOSTNAME_STRICT_HTTPS=false`
- Configurar `KC_HTTP_ENABLED=true`
- Actualizar `ssl_required=NONE` en la base de datos

### 3. Uso de Helm con Múltiples Archivos de Valores

Es importante usar **todos** los archivos de valores en el upgrade:
```bash
# ❌ INCORRECTO (faltan componentes)
helm upgrade portal . -f values-ovh-hosts-portal.yaml -n portal

# ✅ CORRECTO (igual que install)
helm upgrade portal . -f values-adopter-portal.yaml -f values-ovh-hosts-portal.yaml -n portal
```

### 4. Jobs de Kubernetes para Tareas de Administración

Los Jobs de Kubernetes son ideales para:
- Migraciones de base de datos
- Configuraciones post-despliegue
- Tareas de mantenimiento
- Usan secrets de forma segura
- Se pueden re-ejecutar fácilmente

---

## Aplicación en Otros Entornos

Para aplicar estas correcciones en un nuevo entorno:

### 1. Preparar el archivo de valores

Edita tu archivo de valores (ej: `values-nuevo-entorno.yaml`):

```yaml
centralidp:
  keycloak:
    ingress:
      enabled: true
      ingressClassName: "nginx"
      hostname: "centralidp.TU_IP.nip.io"  # ← Cambiar IP
      tls: false
    extraEnvVars:
      - name: KEYCLOAK_FRONTEND_URL
        value: "http://centralidp.TU_IP.nip.io/auth/"  # ← Cambiar IP
      - name: KEYCLOAK_PROXY_ADDRESS_FORWARDING
        value: "true"
      - name: KC_SPI_STICKY_SESSION_ENCODER_INFINISPAN_SHOULD_ATTACH_ROUTE
        value: "false"
      - name: KEYCLOAK_PRODUCTION
        value: "false"
      - name: KC_HTTP_ENABLED
        value: "true"
      # Variables críticas para proxy
      - name: KC_HOSTNAME_STRICT
        value: "false"
      - name: KC_HOSTNAME_STRICT_HTTPS
        value: "false"
      - name: KC_PROXY
        value: "edge"
      - name: KC_HOSTNAME
        value: "centralidp.TU_IP.nip.io"  # ← Cambiar IP
      - name: KC_HOSTNAME_PATH
        value: "/auth/"

sharedidp:
  keycloak:
    ingress:
      enabled: true
      ingressClassName: "nginx"
      hostname: "sharedidp.TU_IP.nip.io"  # ← Cambiar IP
      tls: false
    extraEnvVars:
      - name: KEYCLOAK_FRONTEND_URL
        value: "http://sharedidp.TU_IP.nip.io/auth/"  # ← Cambiar IP
      - name: KEYCLOAK_PROXY_ADDRESS_FORWARDING
        value: "true"
      - name: KC_SPI_STICKY_SESSION_ENCODER_INFINISPAN_SHOULD_ATTACH_ROUTE
        value: "false"
      - name: KEYCLOAK_PRODUCTION
        value: "false"
      - name: KC_HTTP_ENABLED
        value: "true"
      # Variables críticas para proxy
      - name: KC_HOSTNAME_STRICT
        value: "false"
      - name: KC_HOSTNAME_STRICT_HTTPS
        value: "false"
      - name: KC_PROXY
        value: "edge"
      - name: KC_HOSTNAME
        value: "sharedidp.TU_IP.nip.io"  # ← Cambiar IP
      - name: KC_HOSTNAME_PATH
        value: "/auth/"
```

### 2. Desplegar con Helm

```bash
helm install portal . \
  -f values-adopter-portal.yaml \
  -f values-nuevo-entorno.yaml \
  -n portal \
  --create-namespace
```

### 3. Aplicar el Job para SSL

```bash
# Aplicar el Job
kubectl apply -f charts/umbrella/fix-https-requirement-job.yaml

# Esperar a que complete
kubectl wait --for=condition=complete job/fix-https-requirement -n portal --timeout=120s

# Ver logs
kubectl logs job/fix-https-requirement -n portal

# Reiniciar Keycloak
kubectl delete pod -n portal -l app.kubernetes.io/name=centralidp
kubectl delete pod -n portal -l app.kubernetes.io/name=sharedidp
```

### 4. Verificar el acceso

Accede a:
- http://centralidp.TU_IP.nip.io/auth/
- http://sharedidp.TU_IP.nip.io/auth/

---

## Referencias

### Documentación de Keycloak
- [Keycloak Server Configuration](https://www.keycloak.org/server/configuration)
- [Keycloak Proxy Configuration](https://www.keycloak.org/server/reverseproxy)
- [Keycloak Hostname Configuration](https://www.keycloak.org/server/hostname)

### Comandos Útiles de Kubernetes
```bash
# Ver todos los recursos en el namespace
kubectl get all -n portal

# Describir un recurso
kubectl describe <tipo> <nombre> -n portal

# Ver logs en tiempo real
kubectl logs -f <pod-name> -n portal

# Ejecutar comando en un pod
kubectl exec -it <pod-name> -n portal -- <comando>

# Obtener YAML de un recurso
kubectl get <tipo> <nombre> -n portal -o yaml

# Editar un recurso directamente
kubectl edit <tipo> <nombre> -n portal
```

---

## Conclusión

Los problemas encontrados son comunes en despliegues de Keycloak detrás de proxies/ingress en entornos sin HTTPS. Las soluciones implementadas son:

1. **Configuración adecuada de variables de entorno** para proxy y hostname
2. **Actualización de la base de datos** para permitir conexiones HTTP

Ambas soluciones están documentadas y pueden replicarse fácilmente en otros entornos siguiendo esta guía.

---

**Documento generado:** 26 de enero de 2026  
**Autor:** Documentación de despliegue Tractus-X  
**Versión:** 1.0
