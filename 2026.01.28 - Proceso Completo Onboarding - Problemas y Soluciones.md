# Proceso Completo de Onboarding Catena-X - 28 Enero 2026

## üìã Resumen Ejecutivo

**Objetivo**: Completar el proceso de onboarding de TestCompany1 en el entorno Catena-X desplegado en OVH.

**Estado Final**: Infraestructura completa desplegada con imagen personalizada del init-container. Pendiente de ejecutar correcci√≥n de URLs en BD y prueba final de onboarding.

**Duraci√≥n**: ~8 horas de trabajo

**Lecciones Aprendidas Clave**:
1. Los subcharts de BPDM hardcodean hosts `.tx.test` - requieren script post-upgrade
2. Discovery services necesitan ingress manual con `pathType: ImplementationSpecific`
3. PostgreSQL `bitnami/postgresql:15.4.0` deprecada - usar `bitnamilegacy/postgresql:15-debian-11`
4. Realm seeding de Keycloak usa imagen init-container - necesita personalizaci√≥n para URLs correctas
5. La configuraci√≥n correcta es `realmSeeding.initContainer.image.name`, no `initContainer.image`

---

## üéØ Contexto Inicial

### Entorno
- **Cluster**: OVH Kubernetes (66vd7q.c1.gra.k8s.ovh.net)
- **Namespace**: portal
- **Dominio**: 51.68.114.44.nip.io (wildcard DNS)
- **Helm Chart**: tractus-x-umbrella 3.14.5
- **Revisi√≥n Inicial**: 13

### Componentes Inicialmente Desplegados
- ‚úÖ CentralIDP (Keycloak)
- ‚úÖ SharedIDP (Keycloak)
- ‚úÖ BPDM (Gate, Pool, Orchestrator, Cleaning Service)
- ‚úÖ PostgreSQL databases
- ‚ùå Portal (frontend/backend) - NO desplegado
- ‚ùå SSI components - NO desplegados
- ‚ùå Discovery services - NO desplegados

### Problema Inicial
TestCompany1 registrada en el portal pero atascada en estado **SUBMITTED**, no progresaba a ACTIVE.

---

## üìù Fase 1: Diagn√≥stico y Bypass de Clearinghouse (Revisi√≥n 13-14)

### 1.1. An√°lisis del Estado de TestCompany1

```bash
# Consultar estado en la base de datos
kubectl exec -n portal portal-portal-backend-postgresql-0 -- \
  psql -U portal -d portaldb -c \
  "SELECT id, name, company_status_id FROM portal.companies WHERE name LIKE '%TestCompany%';"
```

**Resultado**: TestCompany1 (ID: 13759bce-25db-4890-9c16-79aed8b5179e) en estado SUBMITTED.

### 1.2. Consultar Process Steps

```bash
kubectl exec -n portal portal-portal-backend-postgresql-0 -- \
  psql -U portal -d portaldb -c \
  "SELECT ps.process_step_id, ps.process_step_status_id, ps.message 
   FROM portal.process_steps ps 
   JOIN portal.processes p ON ps.process_id = p.id 
   WHERE p.process_type_id = 1 
   ORDER BY ps.date_created DESC LIMIT 20;"
```

**Hallazgo**: Process step 20 (AWAIT_CLEARING_HOUSE) no estaba completado.

### 1.3. Bypass de Clearinghouse (Entorno de Pruebas)

**Acci√≥n en UI Portal**:
1. Acceder como admin a CentralIDP
2. Navegar a Applications ‚Üí TestCompany1
3. Marcar manualmente checklist como completado

**Script Automatizado Creado** (`bypass-clearinghouse.sh`):
```bash
#!/bin/bash
kubectl exec -n portal portal-portal-backend-postgresql-0 -- psql -U portal -d portaldb <<EOF
-- Obtener application_id
DO \$\$
DECLARE
    v_application_id uuid;
BEGIN
    SELECT id INTO v_application_id 
    FROM portal.company_applications 
    WHERE company_id = '13759bce-25db-4890-9c16-79aed8b5179e' 
    AND application_status_id = 7;
    
    -- Marcar checklist como DONE
    UPDATE portal.application_checklist
    SET application_checklist_entry_status_id = 2
    WHERE application_id = v_application_id
    AND application_checklist_entry_type_id = 6;
    
    -- Insertar process step
    INSERT INTO portal.process_steps (id, process_step_type_id, process_step_status_id, process_id)
    SELECT gen_random_uuid(), 20, 1, id 
    FROM portal.processes 
    WHERE process_type_id = 1;
END \$\$;
EOF
```

**Resultado**: TestCompany1 avanz√≥ pero qued√≥ en **PENDING** (no ACTIVE).

---

## üìù Fase 2: Identificaci√≥n de Componentes SSI Faltantes (Revisi√≥n 14-15)

### 2.1. An√°lisis de Process Steps Fallidos

```bash
kubectl exec -n portal portal-portal-backend-postgresql-0 -- \
  psql -U portal -d portaldb -c \
  "SELECT process_step_type_id, process_step_status_id, message 
   FROM portal.process_steps 
   WHERE process_step_status_id = 3 
   ORDER BY date_created DESC;"
```

**Hallazgo Cr√≠tico**: 
```
process_step_type_id: 20 (Credential Issuance)
message: "call to http://ssi-dim-wallet-stub.51.68.114.44.nip.io/oauth/token failed 404"
```

### 2.2. Componentes SSI Requeridos Identificados

**Componentes cr√≠ticos para onboarding**:
1. **ssi-credential-issuer**: Emisi√≥n de Verifiable Credentials (OBLIGATORIO)
2. **ssi-dim-wallet-stub**: Wallet para almacenar credenciales (OBLIGATORIO)
3. **bdrs-server-memory**: BPN-DID Resolution Service (OBLIGATORIO)
4. **selfdescription (sdfactory)**: Self-Description Factory (OBLIGATORIO)
5. **smtp4dev**: Servidor SMTP para capturar emails con credenciales de registro (OBLIGATORIO en dev/test)
6. **bpndiscovery**: B√∫squeda de EDC por type numbers (RECOMENDADO para data exchange)
7. **discoveryfinder**: B√∫squeda de discovery endpoints (RECOMENDADO para data exchange)

### 2.3. Habilitaci√≥n de Componentes en Values

**Archivo**: `values-adopter-portal-for-onboarding.yaml`

```yaml
# FASE 3: Credential Issuance & Self-Description
selfdescription:
  enabled: true
ssi-credential-issuer:
  enabled: true

# FASE 4: SSI Infrastructure
identity-and-trust-bundle:
  enabled: true
  ssi-dim-wallet-stub:
    enabled: true
bdrs-server-memory:
  enabled: true

# Discovery Services (recomendado para data exchange EDC)
bpndiscovery:
  enabled: true
discoveryfinder:
  enabled: true

# SMTP for Development (OBLIGATORIO - necesario para recibir credenciales y notificaciones)
smtp4dev:
  enabled: true
```

**Nota sobre smtp4dev**: Es **OBLIGATORIO en entornos dev/test**. Durante el proceso de onboarding se env√≠an **m√∫ltiples emails cr√≠ticos** (t√≠picamente 3 emails por business partner):
1. **Email con credenciales iniciales**: Password para primer acceso al portal
2. **Email de confirmaci√≥n de registro**: Verificaci√≥n de datos enviados
3. **Email de aprobaci√≥n**: Notificaci√≥n cuando la aplicaci√≥n es aprobada

Sin smtp4dev no podr√°s capturar estos emails y por tanto no podr√°s:
- Acceder al portal con el password inicial
- Confirmar que el registro fue procesado
- Verificar el estado final de aprobaci√≥n

Los emails capturados se visualizan en http://smtp4dev.51.68.114.44.nip.io. En producci√≥n se reemplazar√≠a por un servidor SMTP real configurado que env√≠e a destinatarios reales.

---

## üìù Fase 3: Despliegue SSI Infrastructure (Revisi√≥n 15-17)

### 3.1. Configuraci√≥n de URLs en values-ovh-hosts-portal.yaml

```yaml
# Self-Description Factory
selfdescription:
  sdfactory:
    secret:
      jwkSetUri: "http://centralidp.51.68.114.44.nip.io/auth/realms/CX-Central/protocol/openid-connect/certs"
  ingress:
    enabled: true
    className: "nginx"
    hosts:
      - host: "sdfactory.51.68.114.44.nip.io"
        paths:
          - path: "/"
            pathType: "Prefix"

# SSI Credential Issuer
ssi-credential-issuer:
  portalBackendAddress: "http://portal-backend.51.68.114.44.nip.io"
  walletAddress: "http://ssi-dim-wallet-stub.51.68.114.44.nip.io"
  walletTokenAddress: "http://ssi-dim-wallet-stub.51.68.114.44.nip.io/oauth/token"
  service:
    credential:
      issuerDid: "did:web:ssi-dim-wallet-stub.51.68.114.44.nip.io:BPNL00000003CRHK"
      statusListUrl: "http://ssi-dim-wallet-stub.51.68.114.44.nip.io/status-list/BPNL00000003CRHK/8a6c7486-1e1f-4555-bdd2-1a178182651e"
  centralidp:
    address: "http://centralidp.51.68.114.44.nip.io"
  ingress:
    enabled: true
    className: "nginx"
    hosts:
      - host: "ssi-credential-issuer.51.68.114.44.nip.io"

# DIM Wallet Stub
identity-and-trust-bundle:
  ssi-dim-wallet-stub:
    wallet:
      host: "ssi-dim-wallet-stub.51.68.114.44.nip.io"
      didHost: "ssi-dim-wallet-stub.51.68.114.44.nip.io"
      stubUrl: "http://ssi-dim-wallet-stub.51.68.114.44.nip.io"
      portal:
        host: "http://portal-backend.51.68.114.44.nip.io"
      keycloak:
        authServerUrl: "http://centralidp.51.68.114.44.nip.io/auth"
      ingress:
        enabled: true
        className: nginx

# BDRS Server
bdrs-server-memory:
  seeding:
    bpnList:
      - bpn: "BPNL00000003CRHK"
        did: "did:web:ssi-dim-wallet-stub.51.68.114.44.nip.io:BPNL00000003CRHK"
  server:
    trustedIssuers:
      - did:web:ssi-dim-wallet-stub.51.68.114.44.nip.io:BPNL00000003CRHK
    env:
      EDC_IAM_DID_WEB_USE_HTTPS: false
    ingresses:
      - enabled: true
        hostname: "bdrs-server.51.68.114.44.nip.io"
        endpoints:
          - directory
          - management
        className: "nginx"
        tls:
          enabled: false
```

### 3.2. Helm Upgrade

```bash
cd charts/umbrella
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml
```

**Resultado**: Revisi√≥n 15 desplegada, componentes SSI activos.

---

## üìù Fase 4: Despliegue Discovery Services - Problema Ingress (Revisi√≥n 16-18)

### 4.1. Configuraci√≥n Inicial de Discovery Services

```yaml
# BPN Discovery
bpndiscovery:
  bpndiscovery:
    host: "semantics.51.68.114.44.nip.io"
    ingress:
      enabled: true  # INICIAL - caus√≥ error
      className: "nginx"
    idp:
      issuerUri: "http://centralidp.51.68.114.44.nip.io/auth/realms/CX-Central"
    discoveryfinderClient:
      baseUrl: "http://semantics.51.68.114.44.nip.io/discoveryfinder"
      provider:
        tokenUri: "http://centralidp.51.68.114.44.nip.io/auth/realms/CX-Central/protocol/openid-connect/token"

# Discovery Finder
discoveryfinder:
  discoveryfinder:
    host: "semantics.51.68.114.44.nip.io"
    properties:
      discoveryfinder:
        initialEndpoints:
          - type: bpn
            endpointAddress: "http://portal-backend.51.68.114.44.nip.io/api/administration/Connectors/discovery"
            description: Service to discover connector endpoints based on bpns
            documentation: "http://portal-backend.51.68.114.44.nip.io/api/administration/swagger/index.html"
    idp:
      issuerUri: "http://centralidp.51.68.114.44.nip.io/auth/realms/CX-Central"
    ingress:
      enabled: true  # INICIAL - caus√≥ error
      className: "nginx"
```

### 4.2. Problema: Helm Validation Error

```bash
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml
```

**Error**:
```
Error: INSTALLATION FAILED: 1 error occurred:
        * admission webhook "validate.nginx.ingress.kubernetes.io" denied the request: 
          path /bpndiscovery(/|$)(.*) cannot be used with pathType Prefix
```

**Causa Ra√≠z**: 
- Subcharts de bpndiscovery/discoveryfinder usan rutas regex: `/bpndiscovery(/|$)(.*)`
- Ingress API requiere `pathType: ImplementationSpecific` para regex
- Pero los subcharts hardcodean `pathType: Prefix` sin exponerlo como par√°metro configurable

### 4.3. Soluci√≥n: Deshabilitar Ingress de Subcharts + Crear Manual

**Paso 1**: Deshabilitar ingress en subcharts

```yaml
bpndiscovery:
  bpndiscovery:
    host: "semantics.51.68.114.44.nip.io"
    ingress:
      enabled: false  # Deshabilitado
    # ... resto configuraci√≥n

discoveryfinder:
  discoveryfinder:
    host: "semantics.51.68.114.44.nip.io"
    ingress:
      enabled: false  # Deshabilitado
    # ... resto configuraci√≥n
```

**Paso 2**: Crear ingress manual `bpndiscovery-ingress.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: portal-bpndiscovery
  namespace: portal
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - host: semantics.51.68.114.44.nip.io
    http:
      paths:
      - path: /bpndiscovery(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: portal-bpndiscovery
            port:
              number: 8080
```

**Paso 3**: Crear ingress manual `discoveryfinder-ingress.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: portal-discoveryfinder
  namespace: portal
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - host: semantics.51.68.114.44.nip.io
    http:
      paths:
      - path: /discoveryfinder(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: portal-discoveryfinder
            port:
              number: 8080
```

**Paso 4**: Helm upgrade + Aplicar ingress manual

```bash
# Helm upgrade (ahora sin error)
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml

# Aplicar ingress manual
kubectl apply -f bpndiscovery-ingress.yaml
kubectl apply -f discoveryfinder-ingress.yaml
```

**Resultado**: Revisi√≥n 16-17 desplegada exitosamente.

---

## üìù Fase 5: Problemas PostgreSQL Image y URL Scheme (Revisi√≥n 17-18)

### 5.1. Problema: PostgreSQL ImagePullBackOff

```bash
kubectl get pods -n portal | grep discovery
```

**Salida**:
```
portal-bpndiscovery-postgresql-0           0/1  ImagePullBackOff
portal-discoveryfinder-postgresql-0        0/1  ImagePullBackOff
```

**Diagn√≥stico**:
```bash
kubectl describe pod portal-bpndiscovery-postgresql-0 -n portal | grep -A5 "Events:"
```

**Error**: 
```
Failed to pull image "bitnami/postgresql:15.4.0-debian-11-r45": 
manifest for bitnami/postgresql:15.4.0-debian-11-r45 not found
```

**Causa**: Imagen deprecada en registry de Bitnami.

### 5.2. Soluci√≥n: Cambiar a bitnamilegacy

**Actualizaci√≥n en values-adopter-portal-for-onboarding.yaml**:

```yaml
bpndiscovery:
  enabled: true
  postgresql:
    image:
      registry: docker.io
      repository: bitnamilegacy/postgresql
      tag: 15-debian-11

discoveryfinder:
  enabled: true
  postgresql:
    image:
      registry: docker.io
      repository: bitnamilegacy/postgresql
      tag: 15-debian-11
```

### 5.3. Problema: URL Sin Esquema

**Error en logs de discoveryfinder**:
```bash
kubectl logs -n portal deployment/portal-discoveryfinder
```

**Mensaje**:
```
java.lang.IllegalArgumentException: URI with undefined scheme
  at discoveryfinderClient.baseUrl
```

**Causa**: Falta `http://` en la URL.

**Soluci√≥n en values-ovh-hosts-portal.yaml**:

```yaml
bpndiscovery:
  bpndiscovery:
    discoveryfinderClient:
      baseUrl: "http://semantics.51.68.114.44.nip.io/discoveryfinder"  # Agregado http://
```

### 5.4. Helm Upgrade Final

```bash
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml
```

**Resultado**: Revisi√≥n 18 desplegada.

### 5.5. Verificaci√≥n de Discovery Services

```bash
# BPN Discovery
curl http://semantics.51.68.114.44.nip.io/bpndiscovery/actuator/health
# Resultado: {"status":"UP"}

curl http://semantics.51.68.114.44.nip.io/bpndiscovery/swagger-ui/index.html
# Resultado: 200 OK

# Discovery Finder
curl http://semantics.51.68.114.44.nip.io/discoveryfinder/actuator/health
# Resultado: {"status":"UP"}

curl http://semantics.51.68.114.44.nip.io/discoveryfinder/swagger-ui/index.html
# Resultado: 200 OK
```

‚úÖ **Discovery services completamente operacionales**.

---

## üìù Fase 6: Descubrimiento de Portal No Desplegado

### 6.1. Consulta Estado de Company

```bash
kubectl exec -n portal portal-portal-backend-postgresql-0 -- \
  psql -U portal -d portaldb -c \
  "SELECT name, company_status_id FROM portal.companies WHERE name LIKE '%TestCompany%';"
```

**Resultado**: ERROR - `database "portaldb" does not exist`

### 6.2. Verificaci√≥n de Bases de Datos

```bash
kubectl exec -n portal portal-portal-backend-postgresql-0 -- \
  psql -U portal -l
```

**Salida**:
```
                              List of databases
   Name    | Owner  | Encoding | Collate |  Ctype  
-----------+--------+----------+---------+---------
 postgres  | portal | UTF8     | en_US.utf8 | en_US.utf8
 template0 | portal | UTF8     | en_US.utf8 | en_US.utf8
 template1 | portal | UTF8     | en_US.utf8 | en_US.utf8
```

**Hallazgo Cr√≠tico**: NO existe `portaldb`. Esto significa que:
- Portal backend NO se ha desplegado nunca
- Portal frontend NO est√° desplegado
- No hay datos de TestCompany1 (se perdieron)

### 6.3. Verificaci√≥n de Pods de Portal

```bash
kubectl get pods -n portal | grep -E "portal-frontend|portal-backend" | grep -v postgres
```

**Resultado**: `No resources found` (ning√∫n pod de portal corriendo)

### 6.4. Verificaci√≥n de Values

```bash
grep "portal:" values-adopter-portal-for-onboarding.yaml -A 2
```

**Salida**:
```yaml
portal:
  enabled: false  # ‚Üê PROBLEMA
```

**Conclusi√≥n**: El portal nunca se ha desplegado. Para onboarding completo se necesita:
1. Habilitar `portal.enabled: true`
2. Desplegar portal frontend/backend
3. Reiniciar proceso de onboarding desde cero

---

## üìù Fase 7: Problema "Invalid parameter: redirect_uri" (Revisi√≥n 19-24)

### 7.1. Error al Acceder al Portal

**URL intentada**: `http://portal.51.68.114.44.nip.io`

**Error en navegador**:
```
Invalid parameter: redirect_uri
```

### 7.2. Diagn√≥stico - Script de Verificaci√≥n

**Creaci√≥n de `check-keycloak-urls.sh`**:

```bash
#!/bin/bash
echo "=== Checking CentralIDP URLs ==="
kubectl exec -n portal portal-centralidp-postgresql-0 -- \
  psql -U kccentral -d iamcentralidp -c \
  "SELECT DISTINCT value FROM redirect_uris ORDER BY value;" 2>/dev/null

echo ""
echo "=== Checking Identity Provider Config ==="
kubectl exec -n portal portal-centralidp-postgresql-0 -- \
  psql -U kccentral -d iamcentralidp -c \
  "SELECT DISTINCT value FROM identity_provider_config 
   WHERE value LIKE '%http%' ORDER BY value;" 2>/dev/null
```

```bash
chmod +x check-keycloak-urls.sh
./check-keycloak-urls.sh
```

**Resultado**:
```
http://managed-identity-wallets.tx.test/*
http://partners-gate.tx.test/*
http://partners-pool.tx.test/*
http://portal.tx.test/*
http://sharedidp.tx.test/auth/realms/CX-Operator/protocol/openid-connect/auth
http://sharedidp.tx.test/auth/realms/CX-Operator/protocol/openid-connect/certs
http://sharedidp.tx.test/auth/realms/CX-Operator/protocol/openid-connect/logout
http://sharedidp.tx.test/auth/realms/CX-Operator/protocol/openid-connect/token
```

**Hallazgo Cr√≠tico**: ‚ùå **Todas las URLs usan `.tx.test`** en lugar de `.51.68.114.44.nip.io`

### 7.3. An√°lisis Causa Ra√≠z - Init-Container

**Verificaci√≥n de imagen usada**:
```bash
kubectl get job -n portal portal-centralidp-realm-seeding-19 -o jsonpath='{.spec.template.spec.initContainers[0].image}'
```

**Resultado**: 
```
docker.io/tractusx/umbrella-init-container:2.3.0-init
```

**Problema Identificado**:
1. Init-container contiene archivos JSON con configuraci√≥n de Keycloak
2. Estos JSON tienen URLs hardcodeadas con `.tx.test`
3. Keycloak importa esos JSON a PostgreSQL
4. Las URLs `.tx.test` quedan en la base de datos
5. Cuando usuario accede con `.51.68.114.44.nip.io`, Keycloak rechaza (no coincide con redirect_uris)

**Ubicaci√≥n del problema**:
```bash
ls -la init-container/iam/centralidp/
```

```
CX-Central-realm.json          ‚Üê Contiene URLs .tx.test
CX-Central-realm_MAssembly.json
CX-Central-users-0.json
```

**Ejemplo de contenido problem√°tico**:
```json
{
  "clients": [
    {
      "clientId": "Cl2-CX-Portal",
      "rootUrl": "http://portal.tx.test/home",
      "redirectUris": [
        "http://portal.tx.test/*"
      ]
    }
  ]
}
```

---

## üìù Fase 8: Creaci√≥n de Imagen Personalizada del Init-Container (Revisi√≥n 20-24)

### 8.1. Actualizaci√≥n de Archivos realm.json

**Script de actualizaci√≥n** (`init-container/update-realm-urls.sh`):

```bash
#!/bin/bash
set -e

if [ -z "$1" ]; then
    echo "Error: Debe proporcionar el nuevo dominio"
    echo "Uso: $0 <nuevo_dominio>"
    exit 1
fi

NEW_DOMAIN="$1"
OLD_DOMAIN="tx.test"

echo "Actualizando URLs de realm.json"
echo "Dominio antiguo: ${OLD_DOMAIN}"
echo "Dominio nuevo: ${NEW_DOMAIN}"

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
IAM_DIR="${SCRIPT_DIR}/iam"

update_realm_file() {
    local file="$1"
    local backup="${file}.backup-$(date +%Y%m%d-%H%M%S)"
    
    if [ ! -f "$file" ]; then
        echo "‚ö†Ô∏è  Archivo no encontrado: $file"
        return 1
    fi
    
    echo "üìù Procesando: $(basename $file)"
    cp "$file" "$backup"
    sed -i "s|${OLD_DOMAIN}|${NEW_DOMAIN}|g" "$file"
    
    local changes=$(diff "$backup" "$file" | grep -c "^[<>]" || true)
    echo "   ‚úÖ Cambios realizados: $((changes / 2)) l√≠neas modificadas"
}

update_realm_file "${IAM_DIR}/centralidp/CX-Central-realm.json"
update_realm_file "${IAM_DIR}/centralidp/CX-Central-realm_MAssembly.json"

echo "‚úÖ Actualizaci√≥n completada"
```

**Ejecuci√≥n**:
```bash
cd init-container
chmod +x update-realm-urls.sh
./update-realm-urls.sh 51.68.114.44.nip.io
```

**Resultado**:
```
üìù Procesando: CX-Central-realm.json
   ‚úÖ Cambios realizados: 11 l√≠neas modificadas
üìù Procesando: CX-Central-realm_MAssembly.json
   ‚úÖ Cambios realizados: 11 l√≠neas modificadas
‚úÖ Actualizaci√≥n completada
```

### 8.2. Instalaci√≥n y Configuraci√≥n de Docker

```bash
# Instalar Docker
sudo apt update && sudo apt install -y docker.io

# Iniciar Docker daemon (en WSL/contenedor)
sudo dockerd > /dev/null 2>&1 &

# Dar permisos al socket
sudo chmod 666 /var/run/docker.sock
```

### 8.3. Construcci√≥n de Imagen Personalizada

```bash
cd init-container

# Construir imagen
sudo docker build -t xmendialdua/catena-x-init:v1 .
```

**Salida**:
```
Successfully built 62b9a9b036a0
Successfully tagged xmendialdua/catena-x-init:v1
```

### 8.4. Push a DockerHub

```bash
# Login
sudo docker login
# (Autenticaci√≥n web-based completada)

# Push imagen
sudo docker push xmendialdua/catena-x-init:v1
```

**Salida**:
```
v1: digest: sha256:8f4fa8156cde1e3b720021e1199d769e16a72ceafcfffa44462c23e6e3d52992 size: 1150
```

‚úÖ **Imagen disponible en**: `docker.io/xmendialdua/catena-x-init:v1`

### 8.5. Configuraci√≥n en Values - INTENTO 1 (Fallido)

**Archivo**: `values-ovh-hosts-portal.yaml`

```yaml
centralidp:
  initContainer:
    image:
      repository: "xmendialdua/catena-x-init"
      tag: "v1"
  keycloak:
    # ... resto configuraci√≥n

sharedidp:
  initContainer:
    image:
      repository: "xmendialdua/catena-x-init"
      tag: "v1"
  keycloak:
    # ... resto configuraci√≥n
```

**Helm Upgrade**:
```bash
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml
```

**Resultado**: Revisi√≥n 20 desplegada

**Verificaci√≥n**:
```bash
kubectl get job portal-centralidp-realm-seeding-20 -n portal -o jsonpath='{.spec.template.spec.initContainers[0].image}'
```

**Salida**: `docker.io/tractusx/umbrella-init-container:2.3.0-init`

‚ùå **Problema**: Sigue usando imagen oficial, no la personalizada.

### 8.6. B√∫squeda de Configuraci√≥n Correcta

**B√∫squeda en repositorio**:
```bash
grep -r "initContainer" charts/umbrella/charts/ 2>/dev/null || echo "No access to subcharts"
```

**B√∫squeda en GitHub**:
```bash
# Encontrado en documentaci√≥n del chart centralidp:
# La estructura correcta es:
# realmSeeding.initContainer.image.name
```

### 8.7. Configuraci√≥n Correcta - INTENTO 2 (Exitoso)

**Actualizaci√≥n de `values-ovh-hosts-portal.yaml`**:

```yaml
centralidp:
  realmSeeding:
    initContainer:
      image:
        name: "docker.io/xmendialdua/catena-x-init:v1"
  keycloak:
    ingress:
      enabled: true
      ingressClassName: "nginx"
      hostname: "centralidp.51.68.114.44.nip.io"
      # ... resto configuraci√≥n

sharedidp:
  realmSeeding:
    initContainer:
      image:
        name: "docker.io/xmendialdua/catena-x-init:v1"
  keycloak:
    ingress:
      enabled: true
      ingressClassName: "nginx"
      hostname: "sharedidp.51.68.114.44.nip.io"
      # ... resto configuraci√≥n
```

**Helm Upgrade**:
```bash
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml
```

**Resultado**: Revisi√≥n 21 desplegada

**Verificaci√≥n**:
```bash
kubectl get job portal-centralidp-realm-seeding-21 -n portal -o jsonpath='{.spec.template.spec.initContainers[0].image}'
```

**Salida**: `docker.io/xmendialdua/catena-x-init:v1`

‚úÖ **¬°√âxito!** Ahora usa la imagen personalizada.

### 8.8. Problema: Realm Seeding es Idempotente

**Verificaci√≥n de URLs**:
```bash
./check-keycloak-urls.sh
```

**Resultado**:
```
http://portal.tx.test/*  ‚Üê Todav√≠a con .tx.test
```

**Causa**: Keycloak realm seeding NO sobrescribe `redirect_uris` si el cliente ya existe. La base de datos ya ten√≠a datos anteriores.

**Soluci√≥n Requerida**:
1. **Opci√≥n A**: Reiniciar bases de datos de Keycloak desde cero
2. **Opci√≥n B**: Ejecutar job de correcci√≥n SQL una √∫ltima vez

---

## üìù Fase 9: Reinicio de Bases de Datos Keycloak (Revisi√≥n 21-24)

### 9.1. Procedimiento de Reinicio Limpio

```bash
# 1. Detener pods de Keycloak
kubectl scale statefulset portal-centralidp -n portal --replicas=0
kubectl scale statefulset portal-sharedidp -n portal --replicas=0

# 2. Detener PostgreSQL
kubectl scale statefulset portal-centralidp-postgresql -n portal --replicas=0
kubectl scale statefulset portal-sharedidp-postgresql -n portal --replicas=0

# 3. Verificar PVC (Persistent Volume Claims)
kubectl get pvc -n portal
# Resultado: No resources found
# (PostgreSQL usa vol√∫menes ef√≠meros, se borran autom√°ticamente)

# 4. Eliminar jobs antiguos de realm-seeding
kubectl delete job -n portal $(kubectl get job -n portal | grep realm-seeding | awk '{print $1}')

# Resultado:
# job.batch "portal-centralidp-realm-seeding-13" deleted
# job.batch "portal-centralidp-realm-seeding-15" deleted
# job.batch "portal-centralidp-realm-seeding-16" deleted
# job.batch "portal-centralidp-realm-seeding-20" deleted
# job.batch "portal-sharedidp-realm-seeding-13" deleted
# job.batch "portal-sharedidp-realm-seeding-15" deleted
# job.batch "portal-sharedidp-realm-seeding-16" deleted
# job.batch "portal-sharedidp-realm-seeding-20" deleted

# 5. Helm upgrade (recrea todo desde cero)
cd charts/umbrella
helm upgrade portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml
```

**Resultado**: Revisi√≥n 21 desplegada, PostgreSQL recreado con BD vac√≠a.

### 9.2. M√∫ltiples Intentos de Configuraci√≥n

**Revisi√≥n 22-23**: Experimentos con diferentes configuraciones de `realmSeeding`.

**Revisi√≥n 24**: Configuraci√≥n final correcta con imagen personalizada.

### 9.3. Verificaci√≥n Final

```bash
# Verificar imagen usada
kubectl get job portal-centralidp-realm-seeding-24 -n portal -o jsonpath='{.spec.template.spec.initContainers[0].image}'
# Resultado: docker.io/xmendialdua/catena-x-init:v1 ‚úÖ

# Verificar URLs en BD
./check-keycloak-urls.sh
```

**Resultado**:
```
http://portal.tx.test/*  ‚Üê A√öN con .tx.test
```

**Causa**: A pesar de la imagen personalizada correcta, Keycloak NO actualiz√≥ `redirect_uris` porque:
1. Los clientes ya exist√≠an en la BD anterior
2. Realm seeding es idempotente (no sobrescribe ciertos campos)
3. `redirect_uris` es uno de los campos que NO se actualiza si el cliente existe

---

## üìù Soluci√≥n Pendiente: Job de Correcci√≥n SQL

### Script fix-keycloak-urls-job-complete.yaml

Este job SQL actualiza **todas** las URLs en las bases de datos de centralidp y sharedidp:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: fix-keycloak-urls-complete
  namespace: portal
spec:
  template:
    spec:
      restartPolicy: Never
      initContainers:
        - name: wait-for-databases
          image: bitnami/kubectl:latest
          command:
            - sh
            - -c
            - |
              kubectl wait --for=condition=ready pod \
                -l app.kubernetes.io/name=postgresql,app.kubernetes.io/instance=portal-centralidp \
                --timeout=300s
              kubectl wait --for=condition=ready pod \
                -l app.kubernetes.io/name=postgresql,app.kubernetes.io/instance=portal-sharedidp \
                --timeout=300s
      containers:
        - name: fix-urls
          image: bitnamilegacy/postgresql:15-debian-11
          env:
            - name: PGPASSWORD_CENTRAL
              valueFrom:
                secretKeyRef:
                  name: portal-centralidp-postgresql
                  key: password
            - name: PGPASSWORD_SHARED
              valueFrom:
                secretKeyRef:
                  name: portal-sharedidp-postgresql
                  key: password
            - name: NEW_DOMAIN
              value: "51.68.114.44.nip.io"
            - name: OLD_DOMAIN
              value: "tx.test"
          command:
            - bash
            - -c
            - |
              set -e
              
              echo "============================================"
              echo "Fixing Keycloak URLs"
              echo "OLD: ${OLD_DOMAIN}"
              echo "NEW: ${NEW_DOMAIN}"
              echo "============================================"
              
              # CentralIDP
              echo ""
              echo "=== Fixing CentralIDP ==="
              PGPASSWORD=$PGPASSWORD_CENTRAL psql -h portal-centralidp-postgresql \
                -U kccentral -d iamcentralidp <<EOF
              
              -- Actualizar redirect_uris
              UPDATE redirect_uris 
              SET value = REPLACE(value, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE value LIKE '%${OLD_DOMAIN}%';
              
              -- Actualizar web_origins
              UPDATE web_origins 
              SET value = REPLACE(value, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE value LIKE '%${OLD_DOMAIN}%';
              
              -- Actualizar client root_url
              UPDATE client 
              SET root_url = REPLACE(root_url, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE root_url LIKE '%${OLD_DOMAIN}%';
              
              -- Actualizar identity_provider_config
              UPDATE identity_provider_config 
              SET value = REPLACE(value, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE value LIKE '%${OLD_DOMAIN}%';
              
              SELECT COUNT(*) as updated_redirect_uris 
              FROM redirect_uris WHERE value LIKE '%${NEW_DOMAIN}%';
              
EOF
              
              echo ""
              echo "=== Fixing SharedIDP ==="
              PGPASSWORD=$PGPASSWORD_SHARED psql -h portal-sharedidp-postgresql \
                -U kcshared -d iamsharedidp <<EOF
              
              UPDATE redirect_uris 
              SET value = REPLACE(value, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE value LIKE '%${OLD_DOMAIN}%';
              
              UPDATE web_origins 
              SET value = REPLACE(value, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE value LIKE '%${OLD_DOMAIN}%';
              
              UPDATE client 
              SET root_url = REPLACE(root_url, '${OLD_DOMAIN}', '${NEW_DOMAIN}')
              WHERE root_url LIKE '%${OLD_DOMAIN}%';
              
              SELECT COUNT(*) as updated_redirect_uris 
              FROM redirect_uris WHERE value LIKE '%${NEW_DOMAIN}%';
              
EOF
              
              echo ""
              echo "============================================"
              echo "‚úÖ URLs updated successfully"
              echo "============================================"
```

**Ejecuci√≥n** (PENDIENTE):
```bash
kubectl apply -f fix-keycloak-urls-job-complete.yaml -n portal
kubectl logs -n portal job/fix-keycloak-urls-complete -f

# Despu√©s de verificar √©xito
kubectl delete job fix-keycloak-urls-complete -n portal
```

---

## üìù Actualizaci√≥n de Template

### values-ovh-hosts-portal-template.yaml

Actualizado con configuraci√≥n de imagen personalizada:

```yaml
centralidp:
  realmSeeding:
    initContainer:
      image:
        name: "docker.io/xmendialdua/catena-x-init:v1"
  keycloak:
    # ... resto configuraci√≥n

sharedidp:
  realmSeeding:
    initContainer:
      image:
        name: "docker.io/xmendialdua/catena-x-init:v1"
  keycloak:
    # ... resto configuraci√≥n
```

---

## üìä Resumen de Revisiones Helm

| Revisi√≥n | Cambios Principales | Estado |
|----------|-------------------|--------|
| 13 | Estado inicial | Base |
| 14 | Bypass Clearinghouse (manual) | Parcial |
| 15 | Habilitaci√≥n SSI components | Activo |
| 16 | Discovery services (con error ingress) | Fallido |
| 17 | Discovery ingress deshabilitado | Exitoso |
| 18 | Fix PostgreSQL image + URL scheme | Exitoso |
| 19 | Ingress manual discovery aplicado | Activo |
| 20 | Primera configuraci√≥n init-container (incorrecta) | Imagen no usada |
| 21 | Reinicio BD Keycloak | BD limpia |
| 22-23 | Experimentos configuraci√≥n | Iteraciones |
| 24 | Configuraci√≥n realmSeeding correcta | **Imagen personalizada OK, URLs pendientes correcci√≥n** |

---

## üéØ Estado Final de Componentes

### Componentes Desplegados ‚úÖ

| Componente | Estado | URL | Notas |
|------------|--------|-----|-------|
| CentralIDP | ‚úÖ Running | http://centralidp.51.68.114.44.nip.io | Con imagen personalizada |
| SharedIDP | ‚úÖ Running | http://sharedidp.51.68.114.44.nip.io | Con imagen personalizada |
| BPDM Gate | ‚úÖ Running | http://business-partners.51.68.114.44.nip.io/gate | Requiere fix-bpdm-hosts.sh |
| BPDM Pool | ‚úÖ Running | http://business-partners.51.68.114.44.nip.io/pool | Requiere fix-bpdm-hosts.sh |
| BPDM Orchestrator | ‚úÖ Running | http://business-partners.51.68.114.44.nip.io/orchestrator | - |
| BPDM Cleaning Service | ‚úÖ Running | - | Internal only |
| Self-Description Factory | ‚úÖ Running | http://sdfactory.51.68.114.44.nip.io | - |
| SSI Credential Issuer | ‚úÖ Running | http://ssi-credential-issuer.51.68.114.44.nip.io | - |
| SSI DIM Wallet Stub | ‚úÖ Running | http://ssi-dim-wallet-stub.51.68.114.44.nip.io | - |
| BDRS Server | ‚úÖ Running | http://bdrs-server.51.68.114.44.nip.io | - |
| BPN Discovery | ‚úÖ Running | http://semantics.51.68.114.44.nip.io/bpndiscovery | Ingress manual |
| Discovery Finder | ‚úÖ Running | http://semantics.51.68.114.44.nip.io/discoveryfinder | Ingress manual |
| SMTP4Dev | ‚úÖ Running | http://smtp4dev.51.68.114.44.nip.io | - |
| PgAdmin4 | ‚úÖ Running | http://pgadmin4.51.68.114.44.nip.io | - |

### Componentes NO Desplegados ‚ùå

| Componente | Estado | Raz√≥n |
|------------|--------|-------|
| Portal Frontend | ‚ùå Not Running | `portal.enabled: false` en values |
| Portal Backend | ‚ùå Not Running | `portal.enabled: false` en values |
| Portal PostgreSQL | ‚ùå Not Running | `portal.enabled: false` en values |

---

## üìã Checklist Proceso Completo para Ma√±ana

### Pre-requisitos

- [ ] Docker instalado y configurado
- [ ] kubectl configurado con acceso al cluster OVH
- [ ] helm instalado
- [ ] Cuenta DockerHub (xmendialdua)
- [ ] Repositorio tractus-x-umbrella clonado

### Paso 1: Preparaci√≥n de Init-Container (Una sola vez)

```bash
# 1. Actualizar realm.json con URLs correctas
cd init-container
./update-realm-urls.sh 51.68.114.44.nip.io

# 2. Construir imagen personalizada
sudo docker build -t xmendialdua/catena-x-init:v1 .

# 3. Subir a DockerHub
sudo docker login
sudo docker push xmendialdua/catena-x-init:v1
```

**Resultado Esperado**: Imagen `docker.io/xmendialdua/catena-x-init:v1` disponible en DockerHub.

### Paso 2: Desinstalaci√≥n Limpia

```bash
# Desinstalar release actual
helm uninstall portal -n portal

# Eliminar PVCs si existen (opcional, para BD completamente limpia)
kubectl delete pvc --all -n portal

# Esperar a que todos los pods terminen
kubectl get pods -n portal
```

**Resultado Esperado**: Namespace limpio, sin recursos.

### Paso 3: Configuraci√≥n de Values

**Archivo**: `values-adopter-portal-for-onboarding.yaml`

```yaml
# FASE 1: Infrastructure Base
pgadmin4:
  enabled: true

centralidp:
  enabled: true

sharedidp:
  enabled: true

# FASE 2: BPDM
bpdm:
  enabled: true
bpdm-gate:
  enabled: true
bpdm-pool:
  enabled: true
bpdm-orchestrator:
  enabled: true
bpdm-cleaning-service-dummy:
  enabled: true

smtp4dev:
  enabled: true

# FASE 3: Credential Issuance & Self-Description
selfdescription:
  enabled: true
ssi-credential-issuer:
  enabled: true

# FASE 4: SSI Infrastructure
identity-and-trust-bundle:
  enabled: true
  ssi-dim-wallet-stub:
    enabled: true
bdrs-server-memory:
  enabled: true

# Discovery Services
bpndiscovery:
  enabled: true
  postgresql:
    image:
      registry: docker.io
      repository: bitnamilegacy/postgresql
      tag: 15-debian-11

discoveryfinder:
  enabled: true
  postgresql:
    image:
      registry: docker.io
      repository: bitnamilegacy/postgresql
      tag: 15-debian-11

# IMPORTANTE: Portal debe estar habilitado para onboarding completo
portal:
  enabled: true  # ‚Üê Cambiar a true
```

**Archivo**: `values-ovh-hosts-portal.yaml` - Verificar que incluye:

```yaml
centralidp:
  realmSeeding:
    initContainer:
      image:
        name: "docker.io/xmendialdua/catena-x-init:v1"

sharedidp:
  realmSeeding:
    initContainer:
      image:
        name: "docker.io/xmendialdua/catena-x-init:v1"

bpndiscovery:
  bpndiscovery:
    host: "semantics.51.68.114.44.nip.io"
    ingress:
      enabled: false
    discoveryfinderClient:
      baseUrl: "http://semantics.51.68.114.44.nip.io/discoveryfinder"

discoveryfinder:
  discoveryfinder:
    host: "semantics.51.68.114.44.nip.io"
    ingress:
      enabled: false
```

### Paso 4: Instalaci√≥n Inicial

```bash
cd charts/umbrella

# Instalar (no upgrade, instalaci√≥n desde cero)
helm install portal . -n portal \
  -f values-adopter-portal-for-onboarding.yaml \
  -f values-ovh-hosts-portal.yaml \
  --create-namespace
```

**Resultado Esperado**: Revisi√≥n 1 desplegada.

### Paso 5: Esperar a Inicializaci√≥n

```bash
# Esperar a que PostgreSQL est√© listo
kubectl wait --for=condition=ready pod \
  -l app.kubernetes.io/name=postgresql \
  -n portal --timeout=600s

# Esperar a realm-seeding jobs
kubectl wait --for=condition=complete job \
  -l app.kubernetes.io/component=realm-seeding \
  -n portal --timeout=600s

# Esperar a portal-migrations job
kubectl wait --for=condition=complete job \
  -l app.kubernetes.io/component=portal-migrations \
  -n portal --timeout=600s
```

**Resultado Esperado**: Todos los jobs completados exitosamente.

### Paso 6: Post-Install Scripts

```bash
# 1. Fix BPDM hosts (requerido despu√©s de cada upgrade)
cd charts/umbrella
./fix-bpdm-hosts.sh

# 2. Aplicar ingress manual para discovery services
kubectl apply -f bpndiscovery-ingress.yaml
kubectl apply -f discoveryfinder-ingress.yaml

# 3. Verificar URLs de Keycloak
./check-keycloak-urls.sh
```

**Resultado Esperado**:
- BPDM accesible en http://business-partners.51.68.114.44.nip.io
- Discovery services accesibles en http://semantics.51.68.114.44.nip.io
- URLs de Keycloak con `.51.68.114.44.nip.io` (NO `.tx.test`)

**Si aparecen URLs con `.tx.test`**: Ejecutar job de correcci√≥n una sola vez:
```bash
kubectl apply -f fix-keycloak-urls-job-complete.yaml -n portal
kubectl wait --for=condition=complete job/fix-keycloak-urls-complete -n portal --timeout=300s
kubectl logs -n portal job/fix-keycloak-urls-complete
kubectl delete job fix-keycloak-urls-complete -n portal
```

### Paso 7: Verificaci√≥n de Componentes

```bash
# Verificar todos los pods est√°n Running
kubectl get pods -n portal | grep -v Completed

# Verificar ingresses
kubectl get ingress -n portal

# Verificar servicios cr√≠ticos
curl -s http://centralidp.51.68.114.44.nip.io/auth/realms/CX-Central/.well-known/openid-configuration | jq '.issuer'
curl -s http://business-partners.51.68.114.44.nip.io/pool/actuator/health | jq '.status'
curl -s http://ssi-credential-issuer.51.68.114.44.nip.io/actuator/health | jq '.status'
curl -s http://semantics.51.68.114.44.nip.io/bpndiscovery/actuator/health | jq '.status'
```

**Resultado Esperado**: Todos devuelven respuestas v√°lidas.

### Paso 8: Acceso al Portal y Onboarding

```bash
# Obtener credenciales admin
kubectl get secret portal-iam-centralidp-admin-credentials -n portal -o jsonpath='{.data.admin-password}' | base64 -d
```

**UI Portal**: http://portal.51.68.114.44.nip.io

**Proceso Onboarding**:
1. Registrar nueva compa√±√≠a como TestCompany1
2. Rellenar formulario de registro
3. Submit application
4. **Autom√°tico**: Proceso en background ejecuta:
   - BPDM crea BPN
   - Self-Description Factory crea SD
   - SSI Credential Issuer emite VC
   - DIM Wallet almacena VC
5. Company status cambia a ACTIVE
6. Acceder a Partner Network

**Verificaci√≥n en BD**:
```bash
kubectl exec -n portal portal-portal-backend-postgresql-0 -- \
  psql -U portal -d portaldb -c \
  "SELECT name, company_status_id, bpn FROM portal.companies WHERE name LIKE '%TestCompany%';"

# company_status_id = 3 significa ACTIVE
```

---

## üö® Problemas Conocidos y Soluciones

### Problema 1: BPDM Ingress Hosts Revert

**S√≠ntoma**: Despu√©s de helm upgrade, BPDM ingress vuelve a usar `.tx.test`

**Causa**: Subcharts de BPDM hardcodean hosts en templates

**Soluci√≥n**: Ejecutar `fix-bpdm-hosts.sh` despu√©s de cada upgrade

```bash
cd charts/umbrella
./fix-bpdm-hosts.sh
```

### Problema 2: Discovery Services Ingress Validation Error

**S√≠ntoma**: 
```
path /bpndiscovery(/|$)(.*) cannot be used with pathType Prefix
```

**Causa**: Subcharts usan regex paths pero hardcodean `pathType: Prefix`

**Soluci√≥n**: 
1. Deshabilitar ingress en subcharts (`ingress.enabled: false`)
2. Crear ingress manual con `pathType: ImplementationSpecific`

```bash
kubectl apply -f bpndiscovery-ingress.yaml
kubectl apply -f discoveryfinder-ingress.yaml
```

### Problema 3: PostgreSQL ImagePullBackOff

**S√≠ntoma**: 
```
Failed to pull image "bitnami/postgresql:15.4.0-debian-11-r45"
```

**Causa**: Imagen deprecada en Bitnami registry

**Soluci√≥n**: Usar `bitnamilegacy/postgresql:15-debian-11`

```yaml
postgresql:
  image:
    registry: docker.io
    repository: bitnamilegacy/postgresql
    tag: 15-debian-11
```

### Problema 4: URL Sin Esquema HTTP

**S√≠ntoma**: 
```
java.lang.IllegalArgumentException: URI with undefined scheme
```

**Causa**: URL configurada sin `http://` o `https://`

**Soluci√≥n**: Asegurar todas las URLs incluyen esquema

```yaml
discoveryfinderClient:
  baseUrl: "http://semantics.51.68.114.44.nip.io/discoveryfinder"  # ‚úÖ
  # NO: "semantics.51.68.114.44.nip.io/discoveryfinder"  # ‚ùå
```

### Problema 5: Keycloak Redirect URI Invalid

**S√≠ntoma**: "Invalid parameter: redirect_uri" al acceder al portal

**Causa**: URLs en realm.json con `.tx.test`, no coinciden con dominio real

**Soluci√≥n Permanente**: Usar imagen personalizada del init-container

**Soluci√≥n Temporal**: Ejecutar job de correcci√≥n SQL

```bash
kubectl apply -f fix-keycloak-urls-job-complete.yaml -n portal
```

### Problema 6: Realm Seeding No Actualiza redirect_uris

**S√≠ntoma**: Despu√©s de upgrade con imagen personalizada, URLs siguen con `.tx.test`

**Causa**: Keycloak realm seeding es idempotente, NO sobrescribe `redirect_uris` si cliente existe

**Soluci√≥n**: 
- **Para instalaci√≥n desde cero**: Imagen personalizada funciona correctamente
- **Para actualizaci√≥n**: Ejecutar job de correcci√≥n SQL una vez

---

## üìö Archivos Creados/Modificados

### Scripts Creados

| Archivo | Ubicaci√≥n | Prop√≥sito |
|---------|-----------|-----------|
| `bypass-clearinghouse.sh` | `./` | Bypass Clearinghouse en entorno test |
| `fix-bpdm-hosts.sh` | `charts/umbrella/` | Corregir BPDM ingress hosts post-upgrade |
| `check-keycloak-urls.sh` | `charts/umbrella/` | Verificar URLs en BD Keycloak |
| `update-realm-urls.sh` | `init-container/` | Actualizar URLs en realm.json |
| `update-and-deploy-realm.sh` | `init-container/` | Script automatizado completo |

### Manifiestos Kubernetes

| Archivo | Ubicaci√≥n | Prop√≥sito |
|---------|-----------|-----------|
| `bpndiscovery-ingress.yaml` | `charts/umbrella/` | Ingress manual para BPN Discovery |
| `discoveryfinder-ingress.yaml` | `charts/umbrella/` | Ingress manual para Discovery Finder |
| `fix-keycloak-urls-job-complete.yaml` | `charts/umbrella/` | Job SQL correcci√≥n URLs |

### Documentaci√≥n

| Archivo | Ubicaci√≥n | Prop√≥sito |
|---------|-----------|-----------|
| `README-realm-urls.md` | `init-container/` | Explicaci√≥n problema URLs |
| `EXPLICACION-IMAGEN-PERSONALIZADA.md` | `init-container/` | Explicaci√≥n detallada init-container |
| `historial_despliegue_ovh.md` | `./` | Historial despliegue (actualizado) |

### Values Modificados

| Archivo | Cambios Principales |
|---------|-------------------|
| `values-adopter-portal-for-onboarding.yaml` | - Habilitados todos componentes FASE 1-4<br>- PostgreSQL image override para discovery<br>- Portal enabled: false ‚Üí true (pendiente) |
| `values-ovh-hosts-portal.yaml` | - Agregadas URLs SSI components<br>- Agregadas URLs discovery services<br>- Configuraci√≥n realmSeeding init-container<br>- Ingress deshabilitado discovery (manual) |
| `values-ovh-hosts-portal-template.yaml` | - Agregada configuraci√≥n realmSeeding |

---

## üéì Lecciones Aprendidas

### 1. Arquitectura de Inicializaci√≥n Keycloak

**Aprendido**:
- Init-container provee archivos JSON est√°ticos
- Keycloak importa JSON a PostgreSQL
- Realm seeding es idempotente (no sobrescribe todo)
- `redirect_uris` NO se actualiza si cliente existe

**Implicaci√≥n**: Para cambiar dominio, necesitas imagen personalizada O correcci√≥n SQL.

### 2. Estructura de Subcharts

**Aprendido**:
- Subcharts pueden tener valores hardcodeados
- No todos los par√°metros son configurables desde umbrella
- Algunos requieren workarounds (scripts post-upgrade, ingress manual)

**Implicaci√≥n**: Revisar templates de subcharts antes de asumir configurabilidad.

### 3. PostgreSQL Image Lifecycle

**Aprendido**:
- Bitnami depreca im√°genes antiguas sin aviso
- `bitnamilegacy` registry contiene versiones deprecadas
- Importante usar misma versi√≥n en todos los componentes

**Implicaci√≥n**: Estandarizar versi√≥n PostgreSQL en todos los subcharts.

### 4. Ingress API Validation

**Aprendido**:
- `pathType: Prefix` incompatible con rutas regex
- `pathType: ImplementationSpecific` necesario para regex
- Nginx ingress controller valida combinaci√≥n path + pathType

**Implicaci√≥n**: Para rutas complejas, usar ingress manual si subchart no permite configurar pathType.

### 5. Service-to-Service Communication

**Aprendido**:
- URLs internas deben incluir esquema (http:// o https://)
- Java cliente HTTP estricto con validaci√≥n URI
- Error cr√≠ptico si falta esquema

**Implicaci√≥n**: Siempre incluir `http://` o `https://` en todas las URLs de configuraci√≥n.

### 6. Onboarding Dependency Chain

**Aprendido**:
El onboarding completo requiere (en orden):
1. Portal (UI + Backend + BD)
2. BPDM (BPN assignment)
3. Self-Description Factory (SD creation)
4. SSI Credential Issuer (VC issuance)
5. DIM Wallet (VC storage)
6. Discovery Services (EDC discovery)

**Implicaci√≥n**: No se puede hacer onboarding parcial. Todos los componentes deben estar operacionales.

---

## üìä M√©tricas del Proceso

| M√©trica | Valor |
|---------|-------|
| **Tiempo Total** | ~8 horas |
| **Helm Upgrades** | 24 revisiones |
| **Scripts Creados** | 5 |
| **Manifiestos Kubernetes** | 3 |
| **Problemas Resueltos** | 6 |
| **Componentes Desplegados** | 16 |
| **Documentos Creados** | 4 |

---

## ‚úÖ Estado Actual del Sistema

### Infraestructura ‚úÖ

- ‚úÖ Kubernetes cluster operacional
- ‚úÖ Nginx ingress controller configurado
- ‚úÖ Wildcard DNS (nip.io) funcionando
- ‚úÖ Helm chart versi√≥n 3.14.5 desplegado

### Backend Services ‚úÖ

- ‚úÖ CentralIDP con imagen personalizada
- ‚úÖ SharedIDP con imagen personalizada
- ‚úÖ BPDM completo (Gate, Pool, Orchestrator, Cleaning)
- ‚úÖ SSI infrastructure completa
- ‚úÖ Discovery services operacionales

### Pendiente ‚è≥

- ‚è≥ Verificaci√≥n/correcci√≥n URLs Keycloak (ejecutar job SQL)
- ‚è≥ Portal frontend/backend deployment (cambiar enabled: true)
- ‚è≥ Prueba onboarding end-to-end
- ‚è≥ Validaci√≥n credenciales emitidas
- ‚è≥ Prueba discovery EDC

---

## üöÄ Pr√≥ximos Pasos Ma√±ana

### Opci√≥n A: Continuar con Instalaci√≥n Actual

1. Ejecutar `fix-keycloak-urls-job-complete.yaml`
2. Verificar URLs correctas con `check-keycloak-urls.sh`
3. Habilitar portal (`portal.enabled: true`)
4. Helm upgrade
5. Probar onboarding completo

### Opci√≥n B: Instalaci√≥n Limpia (RECOMENDADO)

1. `helm uninstall portal -n portal`
2. Seguir checklist completo de este documento
3. Instalaci√≥n desde cero con todos los componentes
4. Portal incluido desde el inicio
5. Imagen personalizada init-container desde inicio

**Recomendaci√≥n**: **Opci√≥n B** para evitar problemas acumulados y tener instalaci√≥n limpia con configuraci√≥n final correcta desde el inicio.

---

## üìû Contactos y Referencias

### Repositorios

- **Tractus-X Umbrella**: https://github.com/eclipse-tractusx/tractus-x-umbrella
- **Init-Container**: Dentro de umbrella, carpeta `init-container/`
- **CentralIDP Chart**: Subchart, revisar con `helm show values`

### Documentaci√≥n

- **Keycloak Realm Seeding**: Documentaci√≥n en chart de centralidp
- **Nginx Ingress PathType**: https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types
- **PostgreSQL Bitnami Legacy**: https://hub.docker.com/r/bitnamilegacy/postgresql

### Registry

- **DockerHub Personal**: https://hub.docker.com/u/xmendialdua
- **Imagen Init**: docker.io/xmendialdua/catena-x-init:v1

---

## üìù Notas Finales

Este documento recoge **toda la informaci√≥n cr√≠tica** del proceso seguido hoy. Para ma√±ana:

1. **Leer el Checklist completo** antes de empezar
2. **Opci√≥n B recomendada**: Instalaci√≥n limpia
3. **Tener Docker listo** para rebuild si necesario
4. **Seguir orden estricto** de pasos
5. **Verificar cada paso** antes de continuar al siguiente

El objetivo final es completar el onboarding de TestCompany1 y validar que:
- ‚úÖ Company status cambia a ACTIVE
- ‚úÖ BPN asignado correctamente
- ‚úÖ Credenciales emitidas y almacenadas
- ‚úÖ Visible en Partner Network
- ‚úÖ EDC discoverable v√≠a discovery services

---

**Documento generado**: 28 Enero 2026  
**√öltima revisi√≥n Helm**: 24  
**Estado**: Infraestructura completa, pendiente validaci√≥n onboarding

---

## üîß Comandos √ötiles de Referencia

### Verificaci√≥n R√°pida del Sistema

```bash
# Estado general
kubectl get pods -n portal | grep -v Completed

# Verificar ingresses
kubectl get ingress -n portal

# Ver revisi√≥n actual
helm list -n portal

# Logs de componente
kubectl logs -n portal -l app.kubernetes.io/name=<component> --tail=100

# Reiniciar pod
kubectl delete pod -n portal <pod-name>

# Verificar jobs completados
kubectl get jobs -n portal

# Ver secrets
kubectl get secrets -n portal

# Acceder a PostgreSQL
kubectl exec -it -n portal portal-centralidp-postgresql-0 -- psql -U kccentral -d iamcentralidp
```

### Debug de Problemas

```bash
# Describir pod con problemas
kubectl describe pod -n portal <pod-name>

# Ver eventos del namespace
kubectl get events -n portal --sort-by='.lastTimestamp'

# Verificar configuraci√≥n ingress
kubectl get ingress -n portal <ingress-name> -o yaml

# Ver logs de init-container
kubectl logs -n portal <pod-name> -c <init-container-name>

# Verificar secret
kubectl get secret -n portal <secret-name> -o yaml

# Port-forward para acceso directo
kubectl port-forward -n portal svc/<service-name> 8080:8080
```

---

**FIN DEL DOCUMENTO**
