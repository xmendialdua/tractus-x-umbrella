# SoluciÃ³n Completa: Despliegue BPDM en Catena-X Portal
**Fecha:** 27 de Enero de 2026  
**Cluster:** OVH Kubernetes (b2-7 flavor, 3 nodos, 6 vCPUs, 21GB RAM)  
**Objetivo:** Desplegar componentes BPDM (Business Partner Data Management) para onboarding de empresas

---

## ğŸ“‹ Resumen Ejecutivo

Los pods BPDM fallaban continuamente en CrashLoopBackOff debido a **dos problemas independientes**:

1. **URLs de autenticaciÃ³n incorrectas**: ConfigMaps contenÃ­an URLs de test (`tx.test`) en lugar de la IP del LoadBalancer
2. **URLs de inter-comunicaciÃ³n incorrectas**: Servicios BPDM intentaban comunicarse usando URLs externas en lugar de servicios internos del cluster

**Resultado Final:** âœ… 5 pods BPDM funcionando correctamente
```bash
bpdm-postgres-0                                1/1  Running  
portal-bpdm-cleaning-service-dummy-*           1/1  Running  
portal-bpdm-gate-*                             1/1  Running  
portal-bpdm-orchestrator-*                     1/1  Running  
portal-bpdm-pool-*                             1/1  Running  
```

---

## ğŸ” Contexto Inicial

### Arquitectura BPDM
BPDM es un sistema distribuido con **dependencias jerÃ¡rquicas**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestrator      â”‚  (Coordina flujos de trabajo)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
      â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pool   â”‚  â”‚ Cleaning Svc â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gate   â”‚  (API para aplicaciones externas)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Orden de arranque crÃ­tico:**
1. Orchestrator debe estar disponible primero
2. Pool necesita conectar a Orchestrator
3. Gate necesita conectar a Pool y Orchestrator
4. Cleaning Service necesita conectar a Orchestrator

### ConfiguraciÃ³n del Entorno
- **LoadBalancer IP:** `51.68.114.44`
- **DNS wildcard:** `*.51.68.114.44.nip.io`
- **Keycloak:** 2 instancias (centralidp, sharedidp) con PostgreSQL
- **Jobs ejecutados 21-23h antes:**
  - `fix-https-requirement-job.yaml`: ConfigurÃ³ `ssl_required='NONE'` en Keycloak
  - `fix-keycloak-urls-job-complete.yaml`: ActualizÃ³ 8 tipos de URLs en bases de datos Keycloak

---

## ğŸš¨ PROBLEMA 1: Pods en CrashLoopBackOff por URLs Incorrectas

### SÃ­ntomas
```bash
kubectl get pods -n portal | grep bpdm
portal-bpdm-gate-*                    0/1  CrashLoopBackOff
portal-bpdm-pool-*                    0/1  CrashLoopBackOff
portal-bpdm-cleaning-service-dummy-*  0/1  CrashLoopBackOff
```

### DiagnÃ³stico
```bash
kubectl logs -n portal portal-bpdm-gate-* --tail=50
```

**Error encontrado:**
```
java.net.UnknownHostException: centralidp.tx.test
  at java.base/java.net.InetAddress$CachedAddresses.get(Unknown Source)
  ...
Caused by: java.net.UnknownHostException: centralidp.tx.test: 
  Name or service not known
```

### Root Cause
Los **ConfigMaps** generados por Helm contenÃ­an URLs del entorno de test (`tx.test`) en lugar de las URLs reales del LoadBalancer:

```yaml
# âŒ ConfigMap INCORRECTO
auth-server-url: http://centralidp.tx.test/auth
```

**Â¿Por quÃ©?**
- Los charts de Helm tienen valores por defecto con `tx.test`
- Aunque `values-ovh-hosts-portal.yaml` tenÃ­a las URLs correctas, los ConfigMaps ya estaban creados
- Helm **no actualiza ConfigMaps existentes** a menos que se ejecute `helm upgrade`

### DecisiÃ³n CrÃ­tica: Â¿Por quÃ© NO usar `helm upgrade`?

**âŒ Evitamos `helm upgrade` porque:**
1. Los jobs de correcciÃ³n Keycloak (`fix-keycloak-urls-complete`, `fix-https-requirement`) ya se habÃ­an ejecutado con Ã©xito 21-23h antes
2. Un `helm upgrade` podrÃ­a **re-ejecutar los jobs de realm-seeding**
3. Esto sobrescribirÃ­a las correcciones en la base de datos de Keycloak
4. PerderÃ­amos las configuraciones de SSL y URLs ya corregidas

**âœ… SoluciÃ³n elegida:** Patchear ConfigMaps directamente con `kubectl`

---

## ğŸ› ï¸ SOLUCIÃ“N 1: CorrecciÃ³n de ConfigMaps con kubectl + jq

### Intento Fallido: sed
Primero intentamos usar `sed` para editar los ConfigMaps:

```bash
# âŒ ESTE MÃ‰TODO NO FUNCIONÃ“
kubectl get configmap portal-bpdm-orchestrator -n portal -o yaml > temp.yaml
sed -i 's/centralidp.tx.test/centralidp.51.68.114.44.nip.io/g' temp.yaml
kubectl apply -f temp.yaml
```

**Problema:** Aunque `sed` modificaba el archivo, `kubectl apply` no detectaba cambios o tenÃ­a problemas con anotaciones/metadatos.

### SoluciÃ³n Exitosa: kubectl + jq (Pipeline JSON)

**Â¿Por quÃ© funciona jq?**
- Trabaja directamente con JSON (formato nativo de Kubernetes API)
- Garantiza estructura JSON vÃ¡lida
- Actualiza el resourceVersion automÃ¡ticamente
- No tiene problemas con caracteres especiales o formato YAML

**Comando verificado:**
```bash
kubectl get configmap NOMBRE -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("OLD_URL"; "NEW_URL")' \
  | kubectl apply -f -
```

### Pasos Ejecutados

#### 1. Probar el mÃ©todo en Orchestrator (validaciÃ³n)
```bash
# Verificar URL actual
kubectl get configmap portal-bpdm-orchestrator -n portal -o yaml | grep "auth-server-url" | head -1
# Resultado: auth-server-url: http://centralidp.tx.test/auth

# Aplicar cambio con jq
kubectl get configmap portal-bpdm-orchestrator -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("centralidp.tx.test"; "centralidp.51.68.114.44.nip.io") | .data."deployment.yml" |= gsub("business-partners.tx.test"; "business-partners.51.68.114.44.nip.io")' \
  | kubectl apply -f -
# Resultado: configmap/portal-bpdm-orchestrator configured

# Verificar cambio aplicado
kubectl get configmap portal-bpdm-orchestrator -n portal -o yaml | grep "auth-server-url" | head -1
# Resultado: auth-server-url: http://centralidp.51.68.114.44.nip.io/auth
# âœ… Ã‰XITO - El cambio se aplicÃ³ correctamente
```

#### 2. Aplicar a Gate
```bash
kubectl get configmap portal-bpdm-gate -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("centralidp.tx.test"; "centralidp.51.68.114.44.nip.io") | .data."deployment.yml" |= gsub("business-partners.tx.test"; "business-partners.51.68.114.44.nip.io")' \
  | kubectl apply -f -
```

#### 3. Aplicar a Pool
```bash
kubectl get configmap portal-bpdm-pool -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("centralidp.tx.test"; "centralidp.51.68.114.44.nip.io") | .data."deployment.yml" |= gsub("business-partners.tx.test"; "business-partners.51.68.114.44.nip.io")' \
  | kubectl apply -f -
```

#### 4. Aplicar a Cleaning Service
```bash
kubectl get configmap portal-bpdm-cleaning-service-dummy -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("centralidp.tx.test"; "centralidp.51.68.114.44.nip.io") | .data."deployment.yml" |= gsub("business-partners.tx.test"; "business-partners.51.68.114.44.nip.io")' \
  | kubectl apply -f -
```

#### 5. Reiniciar Pods (para cargar nuevos ConfigMaps)
```bash
# Reinicio sistemÃ¡tico uno por uno
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-gate
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-pool
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-orchestrator
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-cleaning-service-dummy
```

### Resultado Esperado
Pods arrancan sin errores de DNS... **PERO siguen fallando**

---

## ğŸš¨ PROBLEMA 2: Dependencias No Disponibles (Problema de ComunicaciÃ³n)

### SÃ­ntomas DespuÃ©s del Primer Fix
```bash
kubectl get pods -n portal | grep bpdm
portal-bpdm-orchestrator-*   1/1  Running          # âœ… OK
portal-bpdm-gate-*           0/1  CrashLoopBackOff # âŒ
portal-bpdm-pool-*           0/1  CrashLoopBackOff # âŒ
portal-bpdm-cleaning-svc-*   0/1  CrashLoopBackOff # âŒ
```

### DiagnÃ³stico Pool
```bash
kubectl logs -n portal portal-bpdm-pool-* --tail=100
```

**Error encontrado:**
```
2026-01-27 09:54:57.859 ERROR [...] o.e.t.b.p.u.PoolServiceStartupListner
Startup failed. Dependencies not ready: Orchestrator Service: Down

java.lang.IllegalStateException: Dependencies not ready: Orchestrator Service: Down
  at org.eclipse.tractusx.bpdm.pool.util.PoolServiceStartupListner.onApplicationEvent
  ...
```

### DiagnÃ³stico Gate
```bash
kubectl logs -n portal portal-bpdm-gate-* --tail=100
```

**Error encontrado:**
```
2026-01-27 09:54:40.757 ERROR [...] o.e.t.b.g.u.GateServiceStartupListener
Startup failed. Dependencies not ready: Pool Service: Down, Orchestrator Service: Down

java.lang.IllegalStateException: Dependencies not ready: Pool Service: Down, Orchestrator Service: Down
  at org.eclipse.tractusx.bpdm.gate.util.GateServiceStartupListener.onApplicationEvent
  ...
```

### AnÃ¡lisis del Problema

**Evidencia 1:** Orchestrator estÃ¡ funcionando correctamente
```bash
kubectl logs -n portal portal-bpdm-orchestrator-* --tail=50
# Logs muestran:
# - Tomcat started on port 8085
# - Health checks respondiendo (200 OK)
# - No errores
```

**Evidencia 2:** Servicios Kubernetes existen
```bash
kubectl get svc -n portal | grep orchestrator
portal-bpdm-orchestrator  ClusterIP  10.3.138.11  <none>  80/TCP
```

**Evidencia 3:** Health endpoint del Orchestrator responde internamente
```bash
kubectl exec -n portal portal-bpdm-orchestrator-* -- wget -O- --timeout=5 http://localhost:8085/actuator/health
# âœ… Responde correctamente
```

### Root Cause

Revisamos las URLs en el ConfigMap de Pool:
```bash
kubectl get configmap portal-bpdm-pool -n portal -o yaml | grep -A5 "orchestrator"
```

**Encontrado:**
```yaml
orchestrator:
  base-url: http://business-partners.51.68.114.44.nip.io/orchestrator
```

**ğŸ”´ PROBLEMA IDENTIFICADO:**

Pool/Gate/Cleaning Service estÃ¡n configurados para comunicarse con Orchestrator usando **URLs externas** (a travÃ©s del Ingress/LoadBalancer):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pool   â”‚ â”€â”€http://business-partners.51.68.114.44.nip.io/orchestratorâ”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                                â”‚
                                                                            â”‚
                                                                            â–¼
                                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                    â”‚ LoadBalancer â”‚
                                                                    â”‚  (Ingress)   â”‚
                                                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                           â”‚
                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚  Orchestrator    â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Â¿Por quÃ© falla esto?**

1. **Ingress aÃºn no estÃ¡ configurado correctamente** (tenÃ­a `tx.test`)
2. **Timeout durante startup:** Pool arranca en ~30 segundos e intenta verificar Orchestrator inmediatamente
3. **Latencia innecesaria:** Salir del cluster y volver a entrar aÃ±ade latencia
4. **Dependencia circular:** Orchestrator puede no estar listo externamente aunque estÃ© corriendo

**âœ… SOLUCIÃ“N CORRECTA:**

Usar **comunicaciÃ³n interna del cluster** (ClusterIP Services):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pool   â”‚ â”€â”€http://portal-bpdm-orchestrator:80â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
                                                     â”‚
                 (TrÃ¡fico interno Kubernetes)        â”‚
                                                     â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚  Orchestrator    â”‚
                                          â”‚  Service (ClIP)  â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas:**
- âœ… **MÃ¡s rÃ¡pido:** Sin latencia de LoadBalancer/Ingress
- âœ… **MÃ¡s fiable:** No depende de Ingress/DNS externos
- âœ… **MÃ¡s seguro:** TrÃ¡fico nunca sale del cluster
- âœ… **MÃ¡s simple:** Usa DNS interno de Kubernetes

---

## ğŸ› ï¸ SOLUCIÃ“N 2: Cambiar URLs a ComunicaciÃ³n Interna

### Conceptos Clave: URLs Internas vs Externas

#### URLs EXTERNAS (para browsers/APIs externas)
```yaml
# En values-ovh-hosts-portal.yaml - portal:
bpdm:
  poolAddress: "http://business-partners.51.68.114.44.nip.io"
  portalGateAddress: "http://business-partners.51.68.114.44.nip.io"
```
**Uso:** Cuando el **Portal frontend** (en el browser del usuario) necesita llamar a BPDM API

#### URLs INTERNAS (para comunicaciÃ³n entre pods)
```yaml
# En values-ovh-hosts-portal.yaml - bpdm-pool:
applicationConfig:
  bpdm:
    orchestrator:
      baseUrl: "http://portal-bpdm-orchestrator:80"
```
**Uso:** Cuando **Pool pod** necesita llamar a **Orchestrator pod**

### Verificar Puertos de Servicios

```bash
kubectl get svc portal-bpdm-orchestrator -n portal -o yaml | grep -E "(port:|targetPort:)"
# Output:
#   - port: 80
#     targetPort: 8085

kubectl get svc portal-bpdm-pool -n portal -o yaml | grep -E "(port:|targetPort:)"
# Output:
#   - port: 80
#     targetPort: 8080
```

### Pasos Ejecutados

#### 1. Corregir ConfigMap de Pool
```bash
kubectl get configmap portal-bpdm-pool -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("http://business-partners.51.68.114.44.nip.io/orchestrator"; "http://portal-bpdm-orchestrator:80")' \
  | kubectl apply -f -

# Output: configmap/portal-bpdm-pool configured
```

#### 2. Corregir ConfigMap de Gate (necesita Pool + Orchestrator)
```bash
kubectl get configmap portal-bpdm-gate -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("http://business-partners.51.68.114.44.nip.io/orchestrator"; "http://portal-bpdm-orchestrator:80") | .data."deployment.yml" |= gsub("http://business-partners.51.68.114.44.nip.io/pool"; "http://portal-bpdm-pool:80")' \
  | kubectl apply -f -

# Output: configmap/portal-bpdm-gate configured
```

#### 3. Corregir ConfigMap de Cleaning Service
```bash
kubectl get configmap portal-bpdm-cleaning-service-dummy -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("http://business-partners.51.68.114.44.nip.io/orchestrator"; "http://portal-bpdm-orchestrator:80")' \
  | kubectl apply -f -

# Output: configmap/portal-bpdm-cleaning-service-dummy configured
```

#### 4. Reiniciar Pods (respetando orden de dependencias)

```bash
# Pool primero (depende de Orchestrator que ya estÃ¡ corriendo)
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-pool
# Esperar ~1 minuto

# Gate despuÃ©s (depende de Pool y Orchestrator)
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-gate
# Esperar ~1 minuto

# Cleaning Service (depende de Orchestrator)
kubectl delete pod -n portal -l app.kubernetes.io/name=bpdm-cleaning-service-dummy
```

#### 5. Verificar Estado Final

```bash
# DespuÃ©s de 3-5 minutos:
kubectl get pods -n portal | grep bpdm

# âœ… RESULTADO EXITOSO:
bpdm-postgres-0                                1/1  Running  0  5h55m
portal-bpdm-cleaning-service-dummy-*           1/1  Running  0  3h39m
portal-bpdm-gate-*                             1/1  Running  0  3h42m
portal-bpdm-orchestrator-*                     1/1  Running  0  3h53m
portal-bpdm-pool-*                             1/1  Running  0  3h43m
```

---

## ğŸ“ Modificaciones en Ficheros

### 1. `values-ovh-hosts-portal.yaml`

**UbicaciÃ³n:** `charts/umbrella/values-ovh-hosts-portal.yaml`

**Cambios realizados:**

```yaml
# ANTES (valores incorrectos que causaban el problema):
bpdm-gate:
  applicationConfig:
    bpdm:
      security:
        authServerUrl: "http://centralidp.51.68.114.44.nip.io/auth"
      pool:
        baseUrl: "http://business-partners.51.68.114.44.nip.io/pool"
      orchestrator:
        baseUrl: "http://business-partners.51.68.114.44.nip.io/orchestrator"

bpdm-pool:
  applicationConfig:
    bpdm:
      security:
        authServerUrl: "http://centralidp.51.68.114.44.nip.io/auth"
```

```yaml
# DESPUÃ‰S (configuraciÃ³n correcta):
# BPDM Configuration
# NOTE: Use internal cluster URLs for inter-service communication
# External URLs via Ingress are only for browser/external API access
bpdm-gate:
  applicationConfig:
    bpdm:
      security:
        authServerUrl: "http://centralidp.51.68.114.44.nip.io/auth"
      pool:
        baseUrl: "http://portal-bpdm-pool:80"            # â† CAMBIADO A URL INTERNA
      orchestrator:
        baseUrl: "http://portal-bpdm-orchestrator:80"    # â† CAMBIADO A URL INTERNA

bpdm-pool:
  applicationConfig:
    bpdm:
      security:
        authServerUrl: "http://centralidp.51.68.114.44.nip.io/auth"
      orchestrator:
        baseUrl: "http://portal-bpdm-orchestrator:80"    # â† AGREGADO

bpdm-cleaning-service-dummy:                              # â† AGREGADO COMPLETO
  applicationConfig:
    bpdm:
      orchestrator:
        baseUrl: "http://portal-bpdm-orchestrator:80"
```

**RazÃ³n del cambio:**
- Asegura que futuros `helm upgrade` generen ConfigMaps con URLs internas
- Documenta la configuraciÃ³n correcta para otros clusters
- Mantiene URLs externas en `portal:` para acceso desde browser

### 2. ConfigMaps (patcheados vÃ­a kubectl, NO editados manualmente)

Estos ConfigMaps fueron modificados dinÃ¡micamente usando `kubectl + jq`:

- `portal-bpdm-gate` (namespace: portal)
- `portal-bpdm-pool` (namespace: portal)
- `portal-bpdm-orchestrator` (namespace: portal)
- `portal-bpdm-cleaning-service-dummy` (namespace: portal)

**Cambios en cada ConfigMap:**
1. `centralidp.tx.test` â†’ `centralidp.51.68.114.44.nip.io`
2. `business-partners.tx.test` â†’ URLs internas del cluster
3. URLs externas â†’ URLs internas (segÃºn el servicio)

---

## ğŸ’¾ Jobs Ejecutados (Timeline Completo)

### Jobs Pre-Existentes (ejecutados 21-23h antes del troubleshooting)

#### 1. `fix-https-requirement-job.yaml`
**CuÃ¡ndo:** ~23 horas antes  
**Estado:** Complete (1/1)  
**PropÃ³sito:** Desactivar requisito de HTTPS en Keycloak para permitir conexiones HTTP

**Cambios en Base de Datos:**
```sql
-- En centralidp database:
UPDATE realm SET ssl_required = 'NONE' WHERE name IN ('CX-Central', 'master');

-- En sharedidp database:
UPDATE realm SET ssl_required = 'NONE' WHERE name IN ('CX-Operator', 'master');
```

**Por quÃ© es necesario:**
- El entorno usa `nip.io` sin certificados SSL
- Keycloak por defecto requiere HTTPS para producciÃ³n
- Sin este cambio, Keycloak rechaza conexiones HTTP

#### 2. `fix-keycloak-urls-job-complete.yaml`
**CuÃ¡ndo:** ~21 horas antes  
**Estado:** Complete (1/1)  
**DuraciÃ³n:** 7 segundos  
**PropÃ³sito:** Actualizar 8 tipos diferentes de URLs en bases de datos Keycloak

**Cambios en Base de Datos (centralidp):**

```sql
-- 1. Client root URLs
UPDATE client 
SET root_url = REPLACE(root_url, 'tx.test', '51.68.114.44.nip.io')
WHERE root_url LIKE '%tx.test%';

-- 2. Client redirect URIs
UPDATE redirect_uris 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';

-- 3. Client web origins
UPDATE web_origins 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';

-- 4. Identity provider config
UPDATE identity_provider_config 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';

-- 5. Client attributes
UPDATE client_attributes 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';

-- 6. Identity provider mappers
UPDATE identity_provider_mapper_config 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';

-- 7. Authentication execution config
UPDATE authenticator_config_entry 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';

-- 8. Realm attributes
UPDATE realm_attribute 
SET value = REPLACE(value, 'tx.test', '51.68.114.44.nip.io')
WHERE value LIKE '%tx.test%';
```

**Los mismos cambios se aplicaron a la base de datos `sharedidp`**

**Por quÃ© NO podÃ­amos volver a ejecutar estos jobs:**
- Las actualizaciones SQL son **idempotentes** solo si las URLs viejas siguen existiendo
- Re-ejecutar podrÃ­a:
  - Intentar hacer cambios sobre URLs ya cambiadas
  - Fallar si los realms se re-seedean con valores por defecto
  - Perder configuraciones manuales posteriores
- Los jobs no tienen lÃ³gica de "check if already done"

### Jobs NO Ejecutados Durante el Troubleshooting

**No se ejecutÃ³ ningÃºn Job nuevo** porque:
1. Los cambios en ConfigMaps se hicieron vÃ­a `kubectl patch`
2. Los cambios en base de datos ya estaban hechos
3. QuerÃ­amos evitar interferencias con los jobs previos

---

## ğŸ—„ï¸ Cambios en Base de Datos

### Base de Datos Modificada
- **PostgreSQL centralidp:** Base de datos de Keycloak Central IDP
- **PostgreSQL sharedidp:** Base de datos de Keycloak Shared IDP
- **PostgreSQL bpdm:** Base de datos de BPDM (migraciones automÃ¡ticas por Flyway)

### Cambios Realizados por Jobs (Pre-Troubleshooting)

Ver secciÃ³n anterior "Jobs Ejecutados" para SQL completo.

**Resumen de cambios:**
- 8 tablas modificadas en `centralidp`
- 8 tablas modificadas en `sharedidp`
- Todas las referencias `*.tx.test` â†’ `*.51.68.114.44.nip.io`
- SSL requirement: `EXTERNAL` â†’ `NONE`

### Cambios Durante Troubleshooting

**âŒ NO se modificÃ³ ninguna base de datos directamente durante el troubleshooting**

**RazÃ³n:**
- Los ConfigMaps son independientes de la base de datos
- Las URLs en ConfigMaps afectan a cÃ³mo las **aplicaciones Spring Boot** se conectan entre sÃ­
- Las URLs en la base de datos afectan a cÃ³mo **Keycloak** redirige a usuarios
- Son dos capas diferentes

---

## ğŸ”§ DespuÃ©s del Incremento de Recursos del Cluster

**NOTA:** Durante este troubleshooting **NO se incrementaron recursos del cluster**. SeguÃ­a siendo:
- **Flavor:** b2-7 (2 vCPUs, 7GB RAM por nodo)
- **Nodos:** 3
- **Total:** 6 vCPUs, 21GB RAM

### Plan de Incremento de Recursos (Pendiente)

**Upgrade planeado:**
- **Flavor actual:** b2-7 (2 vCPUs, 7GB RAM)
- **Flavor destino:** b3-16 (4 vCPUs, 16GB RAM)
- **Total despuÃ©s:** 12 vCPUs, 48GB RAM

**Cambios necesarios en Terraform:**
```hcl
# terraform-ovh/main.tf
node_pool {
  name         = "pool-b3-16"
  flavor_name  = "b3-16"          # â† Cambiar de b2-7
  desired_nodes = 3
  max_nodes     = 4
  min_nodes     = 3
}
```

### Pasos a Ejecutar DESPUÃ‰S del Upgrade de Cluster

#### 1. Verificar que todos los pods se recreen en nuevos nodos
```bash
kubectl get nodes
# Verificar 3 nodos con flavor b3-16

kubectl get pods -n portal -o wide
# Verificar que pods estÃ©n en los nuevos nodos
```

#### 2. NO es necesario re-ejecutar jobs Keycloak
Los cambios en base de datos persisten porque:
- PostgreSQL usa PersistentVolumeClaims (PVCs)
- Los datos sobreviven al upgrade de nodos
- Solo los pods se replanifican, no los volÃºmenes

#### 3. Verificar ConfigMaps tras upgrade
```bash
# Si se hizo helm upgrade durante el upgrade del cluster:
kubectl get configmap portal-bpdm-pool -n portal -o yaml | grep "orchestrator"
```

**Si vuelven a aparecer URLs externas**, repetir los comandos de la **SoluciÃ³n 2**

#### 4. Verificar logs despuÃ©s del upgrade
```bash
kubectl logs -n portal -l app.kubernetes.io/name=bpdm-pool --tail=50
kubectl logs -n portal -l app.kubernetes.io/name=bpdm-gate --tail=50
```

---

## ğŸ¯ Por QuÃ© Hacer Cada Cosa

### Â¿Por quÃ© usar `kubectl + jq` en lugar de `helm upgrade`?

**Razones tÃ©cnicas:**
1. **Preservar jobs completados:** Jobs son inmutables, re-ejecutarlos requiere delete+create
2. **Evitar race conditions:** Jobs de seeding podrÃ­an sobrescribir correcciones en DB
3. **Cambio quirÃºrgico:** Solo modificar lo necesario sin tocar 50+ otros recursos
4. **Reversibilidad:** FÃ¡cil de revertir con otro `jq` si algo falla

**Desventajas del mÃ©todo (documentadas para futuros clusters):**
- Cambios en ConfigMaps se pierden en el prÃ³ximo `helm upgrade`
- Requiere modificar `values-ovh-hosts-portal.yaml` para persistencia
- No aparece en el historial de Helm (`helm history`)

### Â¿Por quÃ© URLs internas para comunicaciÃ³n pod-to-pod?

**Razones de arquitectura:**
1. **Latencia:** Evitar salto innecesario por LoadBalancer (~10-50ms extra)
2. **Fiabilidad:** Independiente de Ingress/DNS externo (puntos de falla adicionales)
3. **Seguridad:** TrÃ¡fico nunca sale del cluster
4. **DNS:** Kubernetes DNS es mÃ¡s rÃ¡pido y estable que DNS pÃºblico
5. **Startup:** Servicios ClusterIP disponibles inmediatamente, Ingress puede tardar

**Ejemplo de diferencia de latencia:**
```
URL Interna:  pool â†’ orchestrator (1-2ms)
URL Externa:  pool â†’ LB â†’ Ingress â†’ orchestrator (15-50ms)
```

### Â¿Por quÃ© reiniciar pods uno por uno?

**Razones operacionales:**
1. **Observabilidad:** Ver quÃ© pod falla sin confusiÃ³n
2. **Dependencias:** Respetar el orden Orchestrator â†’ Pool â†’ Gate
3. **Recursos:** No saturar scheduler con mÃºltiples inicios simultÃ¡neos
4. **Logs:** MÃ¡s fÃ¡cil identificar errores en logs individuales
5. **Rollback:** Si algo falla, solo un pod afectado

### Â¿Por quÃ© `tx.test` estaba en los ConfigMaps originalmente?

**Razones de configuraciÃ³n Helm:**
1. Los charts de Tractus-X tienen valores por defecto para entorno de test
2. `values.yaml` del chart tiene:
   ```yaml
   bpdm:
     pool:
       baseUrl: "http://business-partners.tx.test/pool"
   ```
3. Si no se sobreescribe en tu `values-ovh-hosts-portal.yaml`, usa el default
4. Helm merge: `default values.yaml` + `your values.yaml` = final config

**LecciÃ³n aprendida:**
Siempre verificar ConfigMaps despuÃ©s del deploy inicial:
```bash
kubectl get configmap -n portal -o yaml | grep "tx.test"
```

---

## ğŸ“š Comandos de Referencia RÃ¡pida

### DiagnÃ³stico de Problemas BPDM

```bash
# Ver estado de todos los pods BPDM
kubectl get pods -n portal | grep bpdm

# Ver logs de un pod especÃ­fico
kubectl logs -n portal POD_NAME --tail=100

# Ver logs en tiempo real
kubectl logs -n portal POD_NAME -f

# Describir pod (ver eventos de Kubernetes)
kubectl describe pod -n portal POD_NAME

# Ver todos los ConfigMaps de BPDM
kubectl get configmap -n portal | grep bpdm

# Ver contenido de un ConfigMap
kubectl get configmap CONFIGMAP_NAME -n portal -o yaml

# Buscar URLs incorrectas en ConfigMaps
kubectl get configmap -n portal -o yaml | grep "tx.test"
```

### ModificaciÃ³n de ConfigMaps

```bash
# Template genÃ©rico para cambiar URLs con jq
kubectl get configmap NOMBRE -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("OLD_URL"; "NEW_URL")' \
  | kubectl apply -f -

# Verificar cambio
kubectl get configmap NOMBRE -n portal -o yaml | grep "NEW_URL"

# Rollback (volver a URL anterior)
kubectl get configmap NOMBRE -n portal -o json \
  | jq '.data."deployment.yml" |= gsub("NEW_URL"; "OLD_URL")' \
  | kubectl apply -f -
```

### Restart de Pods

```bash
# Por label (recomendado, reinicia el Deployment completo)
kubectl delete pod -n portal -l app.kubernetes.io/name=COMPONENT_NAME

# Por nombre especÃ­fico (solo ese pod)
kubectl delete pod -n portal POD_NAME

# Restart de Deployment (alternativa)
kubectl rollout restart deployment DEPLOYMENT_NAME -n portal

# Ver labels disponibles
kubectl get pods -n portal --show-labels | grep bpdm
```

### VerificaciÃ³n de Servicios

```bash
# Listar servicios BPDM
kubectl get svc -n portal | grep bpdm

# Ver detalles de un servicio (IP, puertos)
kubectl get svc SERVICE_NAME -n portal -o yaml

# Test de conectividad desde otro pod
kubectl run -it --rm debug --image=busybox --restart=Never -n portal -- \
  wget -O- http://portal-bpdm-orchestrator:80/actuator/health

# Verificar endpoints (pods detrÃ¡s del servicio)
kubectl get endpoints -n portal | grep bpdm
```

### Helm Operations (con precauciÃ³n)

```bash
# Ver valores actuales del release
helm get values portal -n portal

# Ver historial de releases
helm history portal -n portal

# Dry-run de upgrade (ver quÃ© cambiarÃ­a)
helm upgrade portal . -f values-file.yaml --dry-run --debug

# Upgrade real (Â¡cuidado con los jobs!)
helm upgrade portal . -f values-file.yaml -n portal
```

---

## âœ… Checklist para Nuevo Cluster

Usa esta lista para desplegar BPDM en un nuevo cluster sin problemas:

### Pre-Despliegue

- [ ] Cluster con recursos suficientes (mÃ­nimo b2-7, recomendado b3-16)
- [ ] LoadBalancer desplegado y IP asignada
- [ ] DNS wildcard configurado (nip.io o similar)
- [ ] Helm 3.x instalado
- [ ] kubectl configurado y acceso verificado
- [ ] jq instalado (`sudo apt-get install jq`)

### ConfiguraciÃ³n de Values

- [ ] Crear/actualizar `values-ovh-hosts-portal.yaml` con:
  - [ ] `global.domainSuffix` correcto
  - [ ] URLs externas en `portal.bpdm.*` para browser
  - [ ] URLs internas en `bpdm-gate.applicationConfig`
  - [ ] URLs internas en `bpdm-pool.applicationConfig`
  - [ ] URLs internas en `bpdm-cleaning-service-dummy.applicationConfig`

**Ejemplo de configuraciÃ³n correcta:**
```yaml
portal:
  bpdm:
    poolAddress: "http://business-partners.IP.nip.io"        # Externa

bpdm-gate:
  applicationConfig:
    bpdm:
      pool:
        baseUrl: "http://portal-bpdm-pool:80"                # Interna
      orchestrator:
        baseUrl: "http://portal-bpdm-orchestrator:80"        # Interna
```

### Despliegue Inicial

- [ ] Deploy con Helm: `helm install portal . -f values-ovh-hosts-portal.yaml -n portal`
- [ ] Esperar a que Keycloak y PostgreSQL estÃ©n Running
- [ ] Verificar que jobs de seeding completen exitosamente

### CorrecciÃ³n de URLs Keycloak

- [ ] Ejecutar `fix-https-requirement-job.yaml`
- [ ] Verificar job completado: `kubectl get jobs -n portal`
- [ ] Ejecutar `fix-keycloak-urls-job-complete.yaml`
- [ ] Verificar job completado y revisar logs
- [ ] Reiniciar pods de Keycloak:
  ```bash
  kubectl delete pod -n portal -l app.kubernetes.io/name=centralidp
  kubectl delete pod -n portal -l app.kubernetes.io/name=sharedidp
  ```

### VerificaciÃ³n de BPDM

- [ ] Esperar 5 minutos despuÃ©s de deploy
- [ ] Verificar estado de pods: `kubectl get pods -n portal | grep bpdm`
- [ ] Si algÃºn pod en CrashLoopBackOff, revisar logs:
  ```bash
  kubectl logs -n portal POD_NAME --tail=100
  ```

### Si hay problemas con URLs

- [ ] Verificar ConfigMaps: `kubectl get configmap -n portal -o yaml | grep "tx.test"`
- [ ] Si aparece `tx.test`, aplicar correcciones con `jq` (ver secciÃ³n comandos)
- [ ] Reiniciar pods afectados uno por uno
- [ ] Verificar que todos lleguen a Running

### VerificaciÃ³n Final

- [ ] Todos los pods BPDM en Running (puede tardar 3-5 minutos)
- [ ] Logs sin errores: `kubectl logs -n portal -l app.kubernetes.io/name=bpdm-pool --tail=20`
- [ ] Test de conectividad externa:
  ```bash
  curl http://business-partners.IP.nip.io/pool/actuator/health
  curl http://business-partners.IP.nip.io/gate/actuator/health
  ```
- [ ] Test de API (con token OAuth2):
  ```bash
  # Obtener token de Keycloak
  TOKEN=$(curl -X POST "http://centralidp.IP.nip.io/auth/realms/CX-Central/protocol/openid-connect/token" \
    -d "grant_type=client_credentials" \
    -d "client_id=CLIENT_ID" \
    -d "client_secret=CLIENT_SECRET" | jq -r '.access_token')
  
  # Llamar a BPDM API
  curl -H "Authorization: Bearer $TOKEN" \
    http://business-partners.IP.nip.io/pool/v7/business-partners
  ```

---

## ğŸ› Troubleshooting ComÃºn

### Problema: Pods en CrashLoopBackOff con "UnknownHostException"

**DiagnÃ³stico:**
```bash
kubectl logs -n portal POD_NAME | grep UnknownHostException
```

**Causa:** ConfigMaps con URLs `tx.test`

**SoluciÃ³n:** Aplicar correcciones con `kubectl + jq` (ver SoluciÃ³n 1)

---

### Problema: "Dependencies not ready: Orchestrator Service: Down"

**DiagnÃ³stico:**
```bash
kubectl logs -n portal POD_NAME | grep "Dependencies not ready"
```

**Causa:** ConfigMaps con URLs externas en lugar de internas

**SoluciÃ³n:** Aplicar correcciones con URLs internas (ver SoluciÃ³n 2)

---

### Problema: Pods reinician constantemente despuÃ©s de llegar a Running

**DiagnÃ³stico:**
```bash
kubectl get pods -n portal -w  # Watch mode
kubectl describe pod POD_NAME -n portal | grep -A10 "Events:"
```

**Causas posibles:**
1. **Liveness probe fallando:** Puerto incorrecto o endpoint no disponible
2. **OOM (Out of Memory):** Pod necesita mÃ¡s memoria
3. **Problemas de base de datos:** Connection pool agotado

**SoluciÃ³n:**
```bash
# Ver uso de recursos
kubectl top pod POD_NAME -n portal

# Aumentar recursos si es necesario
kubectl edit deployment DEPLOYMENT_NAME -n portal
# Modificar resources.limits y resources.requests
```

---

### Problema: "java.sql.SQLTransientConnectionException: HikariPool - Connection is not available"

**DiagnÃ³stico:**
```bash
kubectl logs -n portal POD_NAME | grep "Connection is not available"
```

**Causa:** PostgreSQL sobrecargado o no suficientes conexiones

**SoluciÃ³n:**
```bash
# Verificar estado de PostgreSQL
kubectl get pods -n portal | grep postgres
kubectl logs -n portal bpdm-postgres-0

# Escalar PostgreSQL verticalmente si es necesario
kubectl edit statefulset bpdm-postgres -n portal
# Aumentar resources.limits.memory
```

---

### Problema: ConfigMaps vuelven a tener URLs incorrectas despuÃ©s de un tiempo

**Causa:** Alguien ejecutÃ³ `helm upgrade` sin los values correctos

**PrevenciÃ³n:**
1. Asegurar que `values-ovh-hosts-portal.yaml` tiene las URLs internas correctas
2. Documentar que siempre se debe usar este archivo en upgrades
3. Script de verificaciÃ³n post-upgrade:

```bash
#!/bin/bash
# check-bpdm-configmaps.sh

echo "Verificando ConfigMaps BPDM..."

if kubectl get configmap -n portal -o yaml | grep "tx.test" >/dev/null 2>&1; then
  echo "âŒ ERROR: Encontradas URLs tx.test en ConfigMaps"
  echo "Ejecutar correcciones:"
  echo "  1. Ver README.md secciÃ³n 'CorrecciÃ³n de ConfigMaps'"
  exit 1
else
  echo "âœ… OK: No se encontraron URLs tx.test"
fi

if kubectl get configmap portal-bpdm-pool -n portal -o yaml | grep "business-partners.51.68.114.44.nip.io/orchestrator" >/dev/null 2>&1; then
  echo "âŒ ERROR: Pool usando URL externa para Orchestrator"
  echo "Debe usar: http://portal-bpdm-orchestrator:80"
  exit 1
else
  echo "âœ… OK: Pool usando URL interna"
fi

echo "âœ… Todas las verificaciones pasaron"
```

---

## ğŸ“Š Diagrama de Arquitectura Final

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OVH Kubernetes Cluster                   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Ingress (nginx)                       â”‚  â”‚
â”‚  â”‚  business-partners.51.68.114.44.nip.io                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                  â”‚                  â”‚              â”‚
â”‚           â–¼                  â–¼                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Gate      â”‚    â”‚    Pool     â”‚    â”‚ Orchestratorâ”‚       â”‚
â”‚  â”‚   :8081     â”‚    â”‚    :8080    â”‚    â”‚    :8085    â”‚       â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚       â”‚
â”‚  â”‚ Acceso      â”‚    â”‚  Acceso     â”‚    â”‚  Acceso     â”‚       â”‚
â”‚  â”‚ Externo     â”‚    â”‚  Externo    â”‚    â”‚  Externo    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â”‚                  â”‚                  â”‚              â”‚
â”‚           â”‚  URLs Internas   â”‚   URLs Internas  â”‚              â”‚
â”‚           â”‚  (ClusterIP)     â”‚   (ClusterIP)    â”‚              â”‚
â”‚           â”‚                  â”‚                  â”‚              â”‚
â”‚           â–¼                  â–¼                  â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚              â”‚
â”‚  â”‚  portal-bpdm-pool:80           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”‚  (Servicio Kubernetes)         â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚           â”‚                                                    â”‚
â”‚           â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚  portal-bpdm-orchestrator:80   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚  (Servicio Kubernetes)         â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚           â”‚                                                    â”‚
â”‚           â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚   bpdm-postgres:5432        â”‚                              â”‚
â”‚  â”‚   (PostgreSQL)              â”‚                              â”‚
â”‚  â”‚   PVC: bpdm-data            â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚   centralidp-keycloak       â”‚                              â”‚
â”‚  â”‚   (AutenticaciÃ³n)           â”‚                              â”‚
â”‚  â”‚   http://centralidp...      â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Leyenda:
  Flechas sÃ³lidas (â”‚ â–¼): ComunicaciÃ³n interna (ClusterIP)
  Flechas externas: Acceso desde Internet (Ingress)
```

---

## ğŸ“– Referencias y DocumentaciÃ³n

### DocumentaciÃ³n Oficial
- [Tractus-X Umbrella Charts](https://github.com/eclipse-tractusx/tractus-x-umbrella)
- [BPDM Documentation](https://eclipse-tractusx.github.io/docs/tutorials/)
- [Kubernetes ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/)
- [Kubernetes Services](https://kubernetes.io/docs/concepts/services-networking/service/)

### Archivos Relacionados en este Repositorio
- `charts/umbrella/values-ovh-hosts-portal.yaml` - ConfiguraciÃ³n principal
- `charts/umbrella/values-adopter-portal-for-onboarding.yaml` - ActivaciÃ³n de componentes
- `fix-https-requirement-job.yaml` - Job de desactivaciÃ³n SSL Keycloak
- `fix-keycloak-urls-job-complete.yaml` - Job de actualizaciÃ³n URLs en DB

### Documentos Generados en este Proyecto
- `2026.01.23 - Init-Container y Realm Seeding - Guia Completa v2.md`
- `2026.01.23 - Result_Check_Keycloak_URLs.md`
- `2026.01.26 - Despliegue Componentes Onboarding - Estado y Proximos Pasos.md`
- `2026.01.26 - Solucion Errores Keycloak - Cookie y HTTPS.md`

---

## âœ¨ Conclusiones y Lecciones Aprendidas

### Principales Aprendizajes

1. **URLs Internas vs Externas son CrÃ­ticas**
   - Las aplicaciones en Kubernetes deben usar URLs internas (ClusterIP) para comunicarse entre pods
   - URLs externas (Ingress) solo para acceso desde fuera del cluster

2. **ConfigMaps No se Actualizan AutomÃ¡ticamente**
   - Helm no actualiza ConfigMaps existentes sin `helm upgrade`
   - Los valores en `values.yaml` no se propagan a ConfigMaps ya creados
   - Necesitas patchear manualmente o hacer upgrade completo

3. **Jobs Son Inmutables**
   - Una vez ejecutados, no se pueden "re-ejecutar" sin borrar y recrear
   - Cuidado con `helm upgrade` que puede intentar recrear jobs
   - Mejor usar `kubectl apply` directo para modificaciones puntuales

4. **Orden de Dependencias Importa**
   - BPDM tiene arquitectura con dependencias: Orchestrator â†’ Pool â†’ Gate
   - Verificar que servicios base estÃ©n disponibles antes de arrancar dependientes
   - Usar health checks y readiness probes correctamente

5. **kubectl + jq es MÃ¡s Seguro que sed para ConfigMaps**
   - JSON es mÃ¡s estructurado que YAML
   - jq garantiza sintaxis vÃ¡lida
   - Menos propenso a errores de formato

### Recomendaciones para Futuros Deploys

1. **Verificar ConfigMaps DespuÃ©s del Deploy Inicial**
   ```bash
   kubectl get configmap -n portal -o yaml | grep -E "(tx.test|INCORRECT_URL)"
   ```

2. **Mantener values-ovh-hosts-portal.yaml Actualizado**
   - Incluir TODAS las URLs internas necesarias
   - Documentar en comentarios quÃ© URLs son internas vs externas
   - Versionar el archivo en Git

3. **Automatizar Verificaciones Post-Deploy**
   - Script que verifique URLs correctas en ConfigMaps
   - Alertas si pods entran en CrashLoopBackOff
   - Health checks de los endpoints BPDM

4. **Documentar Decisiones de Arquitectura**
   - Por quÃ© se usan URLs internas
   - Por quÃ© evitamos helm upgrade en ciertos casos
   - Workflow de modificaciÃ³n de ConfigMaps

5. **Plan de Upgrade de Cluster**
   - Documentar pasos antes/despuÃ©s del upgrade
   - Backup de PVCs importantes (PostgreSQL)
   - VerificaciÃ³n de que ConfigMaps siguen correctos

### PrÃ³ximos Pasos (Para este Cluster)

- [ ] Upgrade de cluster a b3-16 (mÃ¡s recursos)
- [ ] Desplegar Fase 2: smtp4dev
- [ ] Desplegar Fase 3: ssi-credential-issuer
- [ ] Desplegar Fase 4: identity-and-trust-bundle
- [ ] Configurar Ingress de BPDM correctamente
- [ ] Onboarding de empresas: Ikerlan, Orona, Fagor

---

## ğŸ“ Contacto y Soporte

**Autor:** xmendialdua  
**Fecha:** 27 de Enero de 2026  
**Proyecto:** Tractus-X Umbrella - OVH Deployment  

Para preguntas o problemas relacionados con este documento:
- Revisar logs de pods: `kubectl logs -n portal POD_NAME`
- Verificar estado de recursos: `kubectl get all -n portal | grep bpdm`
- Consultar documentaciÃ³n oficial de Tractus-X

---

**ğŸ‰ Â¡Deployment BPDM Completado Exitosamente!**
